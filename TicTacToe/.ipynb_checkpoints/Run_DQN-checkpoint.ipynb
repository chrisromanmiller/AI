{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermiller/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "from mods import multiplayer_tools, TicTacToe,Players, DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "\n",
      "\n",
      "Time since last update 0.9635438919067383\n",
      "Total gradient step runtime: 0.2928659915924072\n",
      "Total buffer time: 0.6700561046600342\n",
      "[[0.         0.         0.01916552]\n",
      " [0.         0.00743866 0.        ]\n",
      " [0.00743747 0.01971388 0.01574934]]\n",
      "Size of replay buffer: 161\n",
      "\n",
      "\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "\n",
      "\n",
      "Time since last update 1.0499870777130127\n",
      "Total gradient step runtime: 0.014540910720825195\n",
      "Total buffer time: 0.7988376617431641\n",
      "[[0.         0.01309419 0.03582752]\n",
      " [0.01392579 0.01734316 0.00580919]\n",
      " [0.02335954 0.03249276 0.02124703]]\n",
      "Size of replay buffer: 360\n",
      "\n",
      "\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "\n",
      "\n",
      "Time since last update 1.095479965209961\n",
      "Total gradient step runtime: 0.0161590576171875\n",
      "Total buffer time: 0.9345769882202148\n",
      "[[0.         0.02300835 0.04798341]\n",
      " [0.02828729 0.03138459 0.01894331]\n",
      " [0.03841591 0.03964424 0.02343333]]\n",
      "Size of replay buffer: 570\n",
      "\n",
      "\n",
      "duplicating session to freeze weights for evaluation...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3cf5a3e19fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m#         if frac_player_loss <= frac_opponent_loss:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mopponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNN_Player\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduplicate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/mods/Players.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, model_sample_s, session, observation_placeholder, duplicate, deterministic)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0;31m# Want to duplicate session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1618\u001b[0m           model_checkpoint_path = sess.run(\n\u001b[1;32m   1619\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0mmodel_checkpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1263\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "from collections import Counter\n",
    "\n",
    "#Define the placeholders\n",
    "#TODO: update placeholders\n",
    "observation_placeholder = tf.placeholder(shape = [None,2, 3,3], dtype = tf.int32, name = \"obs_placeholder\")\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32, name = \"act_placeholder\")\n",
    "\n",
    "#target place holder is r(s,a) + \\gamma \\max_a Q(s',a)\n",
    "target_placeholder = tf.placeholder(shape = [None], dtype = tf.float32, name = \"target_placeholder\")\n",
    "\n",
    "#Define the model and loss function\n",
    "model = DQN.TicTacToe_model(observation_placeholder, scope = \"Q_learn\")\n",
    "update_op = DQN.symbolic_Q_update(model, target_placeholder, action_placeholder)\n",
    "\n",
    "#Start a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Define the players\n",
    "player = Players.NN_Player(model, model, sess, observation_placeholder, duplicate = False)\n",
    "opponent = Players.Random_Player()\n",
    "judge =  Players.NN_Player(model, model, player.session, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "#Define the environment\n",
    "env = TicTacToe.mnk_game()\n",
    "\n",
    "#Load current net\n",
    "temp_file_name = './bot_10_31_q_v2.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "#saver = tf.train.Saver()\n",
    "#saver.restore(sess, temp_file_name)\n",
    "\n",
    "replay_buffer = []\n",
    "first_state = None\n",
    "step = 0\n",
    "tic = time.time()\n",
    "total_buffer_time = 0\n",
    "total_runtime = 0\n",
    "while True:\n",
    "    step += 1\n",
    "\n",
    "    #Collect rollouts\n",
    "    tic_buffer = time.time()\n",
    "    paths, _ = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps = 300)\n",
    "\n",
    "    #Add rollouts to the replay buffer\n",
    "    if len(replay_buffer) > 100000:\n",
    "        replay_buffer = replay_buffer[30000:]\n",
    "    \n",
    "    replay_buffer += paths\n",
    "    #Collect samples from our replay buffer\n",
    "    states, actions, next_states, rewards, masks, not_end_of_path = DQN.sample_paths(replay_buffer, batch_size = 100)\n",
    "    total_buffer_time +=  (time.time() - tic_buffer)\n",
    "\n",
    "    \n",
    "    tic_runtime = time.time()\n",
    "    #Compute target values\n",
    "    target_values = DQN.compute_target_values(judge, next_states, masks, not_end_of_path, rewards, verbose=False)\n",
    "\n",
    "    #Update the network\n",
    "    player.session.run(update_op, feed_dict= {observation_placeholder : states, action_placeholder : actions, target_placeholder : target_values })\n",
    "    total_runtime += (time.time() - tic_runtime)\n",
    "    \n",
    "    if step % 10 ==0:\n",
    "        _, player_batch_winners = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps=1000)\n",
    "        _, judge_batch_winners = multiplayer_tools.batch_rollout(judge, opponent, env, max_time_steps=1000)\n",
    "        frac_player_loss = (player_batch_winners[2]*1.0)/(player_batch_winners[0] + player_batch_winners[1] + player_batch_winners[2])\n",
    "        frac_judge_loss = (judge_batch_winners[2]*1.0)/(judge_batch_winners[0] + judge_batch_winners[1] + judge_batch_winners[2])\n",
    "        if frac_player_loss <= frac_judge_loss:\n",
    "            print(\"replacing \")\n",
    "            judge = Players.NN_Player(model, model, player.session, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "    #Occasionally, test the model and replace it with a previous iteration\n",
    "    if step% 2 ==0:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Time since last update\", time.time() - tic)\n",
    "        print(\"Total gradient step runtime:\",total_runtime)\n",
    "        print(\"Total buffer time:\", total_buffer_time)\n",
    "        total_runtime = 0\n",
    "        total_buffer_time = 0\n",
    "        tic = time.time()\n",
    "        step = 0\n",
    "        \n",
    "        #Print current board distribution\n",
    "        initial_board = np.array(sess.run(model, feed_dict= {observation_placeholder : [np.array([[[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0]]]) == 1] }))\n",
    "        print(np.reshape(initial_board, newshape = (3,3)))\n",
    "\n",
    "#        player.epsilon = 0\n",
    "#        expert_player = Players.Expert_Player()\n",
    "#        child_player = Players.Child_Player()\n",
    "#        _, expert_batch_winners = multiplayer_tools.batch_rollout(player, expert_player, env, max_time_steps=500)\n",
    "#        _, child_batch_winners = multiplayer_tools.batch_rollout(player, child_player, env, max_time_steps=500)\n",
    "#         print(expert_batch_winners)\n",
    "#        expert_batch_percentages = np.array([expert_batch_winners[0], expert_batch_winners[1], expert_batch_winners[2]])*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "#        child_batch_percentages = np.array([child_batch_winners[0], child_batch_winners[1], child_batch_winners[2]])*1.0/(child_batch_winners[0] + child_batch_winners[1] + child_batch_winners[2])\n",
    "#        player.epsilon = .2\n",
    "#        print(\"percentages against expert\", expert_batch_percentages.tolist())\n",
    "#        print(\"percentages against child\", child_batch_percentages.tolist())\n",
    "\n",
    "        print(\"Size of replay buffer:\", len(replay_buffer))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "#         _, player_batch_winners = batch_rollout(player, expert_player, env, max_time_steps=1000)\n",
    "#         frac_player_loss = (player_batch_winners[2]*1.0)/(player_batch_winners[0] + player_batch_winners[1] + player_batch_winners[2])\n",
    "#        print(player_batch_winners)\n",
    "#         _, opponent_batch_winners = batch_rollout(opponent, expert_player, env, max_time_steps=1000)\n",
    "#         frac_opponent_loss = (opponent_batch_winners[2]*1.0)/(opponent_batch_winners[0] + opponent_batch_winners[1] + opponent_batch_winners[2])\n",
    "        \n",
    "        \n",
    "#         if frac_player_loss <= frac_opponent_loss:\n",
    "        opponent = Players.NN_Player(model, model, sess, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
