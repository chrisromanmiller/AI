{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "\n",
    "import multiplayer_tools\n",
    "import tictactoe\n",
    "import Players\n",
    "import Policy_Gradient\n",
    "import DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "\n",
      "\n",
      "Time since last update 44.008421897888184\n",
      "Total gradient step runtime: 0.8389551639556885\n",
      "Total buffer time: 24.888524770736694\n",
      "[[0.560761   0.43686116 0.49018955]\n",
      " [0.39114213 0.5716182  0.44210017]\n",
      " [0.46782494 0.4677683  0.5682844 ]]\n",
      "Size of replay buffer: 8516\n",
      "\n",
      "\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "Destroying NN_Player and Session...\n",
      "Destroying NN_Player and Session...\n",
      "Destroying NN_Player and Session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method NN_Player.__del__ of <Players.NN_Player object at 0x1c36b8a748>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/christophermiller/Documents/GitHub/ai/TicTacToe/venv/Players.py\", line 342, in __del__\n",
      "    self.session.close()\n",
      "AttributeError: 'NN_Player' object has no attribute 'session'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "\n",
      "\n",
      "Time since last update 73.69432401657104\n",
      "Total gradient step runtime: 0.7259688377380371\n",
      "Total buffer time: 42.80161786079407\n",
      "[[0.64592564 0.39873374 0.60020006]\n",
      " [0.64746237 0.68212664 0.6101017 ]\n",
      " [0.57287836 0.44929373 0.66145873]]\n",
      "Size of replay buffer: 16264\n",
      "\n",
      "\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "\n",
      "\n",
      "Time since last update 83.68851709365845\n",
      "Total gradient step runtime: 0.8836643695831299\n",
      "Total buffer time: 51.058441162109375\n",
      "[[0.5963458  0.41495645 0.5998297 ]\n",
      " [0.5910567  0.6559838  0.6007587 ]\n",
      " [0.48566473 0.4288584  0.60331523]]\n",
      "Size of replay buffer: 25099\n",
      "\n",
      "\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "\n",
      "\n",
      "Time since last update 96.85480499267578\n",
      "Total gradient step runtime: 0.9832797050476074\n",
      "Total buffer time: 59.889278411865234\n",
      "[[0.6717441  0.38915992 0.65680444]\n",
      " [0.63755643 0.68411005 0.7043469 ]\n",
      " [0.6074432  0.38868093 0.6777756 ]]\n",
      "Size of replay buffer: 33085\n",
      "\n",
      "\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-edae299fd512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mreplay_buffer\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m#Collect samples from our replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_end_of_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mtotal_buffer_time\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/DQN.py\u001b[0m in \u001b[0;36msample_paths\u001b[0;34m(paths, batch_size)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m#Make the easy lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mobservation_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0maction_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mreward_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mmask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/DQN.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;31m#Make the easy lists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mobservation_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0maction_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mreward_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mmask_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tictactoe \n",
    "tf.reset_default_graph()\n",
    "reload(Policy_Gradient)\n",
    "reload(DQN)\n",
    "reload(Players)\n",
    "reload(tictactoe)\n",
    "from collections import Counter\n",
    "\n",
    "#Define the placeholders\n",
    "#TODO: update placeholders\n",
    "observation_placeholder = tf.placeholder(shape = [None,2, 3,3], dtype = tf.int32, name = \"obs_placeholder\")\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32, name = \"act_placeholder\")\n",
    "\n",
    "#target place holder is r(s,a) + \\gamma \\max_a Q(s',a)\n",
    "target_placeholder = tf.placeholder(shape = [None], dtype = tf.float32, name = \"target_placeholder\")\n",
    "\n",
    "#Define the model and loss function\n",
    "model = Policy_Gradient.TicTacToe_model(observation_placeholder, scope = \"Q_learn\")\n",
    "update_op = DQN.symbolic_Q_update(model, target_placeholder, action_placeholder)\n",
    "\n",
    "#Start a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Define the players\n",
    "player = Players.NN_Player(model, model, sess, observation_placeholder, duplicate = False)\n",
    "opponent = Players.Random_Player()\n",
    "judge =  Players.NN_Player(model, model, player.session, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "#Define the environment\n",
    "env = tictactoe.mnk_game()\n",
    "\n",
    "#Load current net\n",
    "temp_file_name = './bot_10_31_q_v2.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "#saver = tf.train.Saver()\n",
    "#saver.restore(sess, temp_file_name)\n",
    "\n",
    "replay_buffer = []\n",
    "first_state = None\n",
    "step = 0\n",
    "tic = time.time()\n",
    "total_buffer_time = 0\n",
    "total_runtime = 0\n",
    "while True:\n",
    "    step += 1\n",
    "\n",
    "    #Collect rollouts\n",
    "    tic_buffer = time.time()\n",
    "    paths, _ = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps = 300)\n",
    "\n",
    "    #Add rollouts to the replay buffer\n",
    "    if len(replay_buffer) > 100000:\n",
    "        replay_buffer = replay_buffer[30000:]\n",
    "    \n",
    "    replay_buffer += paths\n",
    "    #Collect samples from our replay buffer\n",
    "    states, actions, next_states, rewards, masks, not_end_of_path = DQN.sample_paths(replay_buffer, batch_size = 100)\n",
    "    total_buffer_time +=  (time.time() - tic_buffer)\n",
    "\n",
    "    \n",
    "    tic_runtime = time.time()\n",
    "    #Compute target values\n",
    "    target_values = DQN.compute_target_values(judge, next_states, masks, not_end_of_path, rewards, verbose=False)\n",
    "\n",
    "    #Update the network\n",
    "    player.session.run(update_op, feed_dict= {observation_placeholder : states, action_placeholder : actions, target_placeholder : target_values })\n",
    "    total_runtime += (time.time() - tic_runtime)\n",
    "    \n",
    "    if step % 10 ==0:\n",
    "        _, player_batch_winners = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps=1000)\n",
    "        _, judge_batch_winners = multiplayer_tools.batch_rollout(judge, opponent, env, max_time_steps=1000)\n",
    "        frac_player_loss = (player_batch_winners[2]*1.0)/(player_batch_winners[0] + player_batch_winners[1] + player_batch_winners[2])\n",
    "        frac_judge_loss = (judge_batch_winners[2]*1.0)/(judge_batch_winners[0] + judge_batch_winners[1] + judge_batch_winners[2])\n",
    "        if frac_player_loss <= frac_judge_loss:\n",
    "            print(\"replacing \")\n",
    "            judge = Players.NN_Player(model, model, player.session, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "    #Occasionally, test the model and replace it with a previous iteration\n",
    "    if step% 100 ==0:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Time since last update\", time.time() - tic)\n",
    "        print(\"Total gradient step runtime:\",total_runtime)\n",
    "        print(\"Total buffer time:\", total_buffer_time)\n",
    "        total_runtime = 0\n",
    "        total_buffer_time = 0\n",
    "        tic = time.time()\n",
    "        step = 0\n",
    "        \n",
    "        #Print current board distribution\n",
    "        initial_board = np.array(sess.run(model, feed_dict= {observation_placeholder : [np.array([[[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0]]]) == 1] }))\n",
    "        print(np.reshape(initial_board, newshape = (3,3)))\n",
    "\n",
    "#        player.epsilon = 0\n",
    "#        expert_player = Players.Expert_Player()\n",
    "#        child_player = Players.Child_Player()\n",
    "#        _, expert_batch_winners = multiplayer_tools.batch_rollout(player, expert_player, env, max_time_steps=500)\n",
    "#        _, child_batch_winners = multiplayer_tools.batch_rollout(player, child_player, env, max_time_steps=500)\n",
    "#         print(expert_batch_winners)\n",
    "#        expert_batch_percentages = np.array([expert_batch_winners[0], expert_batch_winners[1], expert_batch_winners[2]])*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "#        child_batch_percentages = np.array([child_batch_winners[0], child_batch_winners[1], child_batch_winners[2]])*1.0/(child_batch_winners[0] + child_batch_winners[1] + child_batch_winners[2])\n",
    "#        player.epsilon = .2\n",
    "#        print(\"percentages against expert\", expert_batch_percentages.tolist())\n",
    "#        print(\"percentages against child\", child_batch_percentages.tolist())\n",
    "\n",
    "        print(\"Size of replay buffer:\", len(replay_buffer))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "#         _, player_batch_winners = batch_rollout(player, expert_player, env, max_time_steps=1000)\n",
    "#         frac_player_loss = (player_batch_winners[2]*1.0)/(player_batch_winners[0] + player_batch_winners[1] + player_batch_winners[2])\n",
    "#        print(player_batch_winners)\n",
    "#         _, opponent_batch_winners = batch_rollout(opponent, expert_player, env, max_time_steps=1000)\n",
    "#         frac_opponent_loss = (opponent_batch_winners[2]*1.0)/(opponent_batch_winners[0] + opponent_batch_winners[1] + opponent_batch_winners[2])\n",
    "        \n",
    "        \n",
    "#         if frac_player_loss <= frac_opponent_loss:\n",
    "        opponent = Players.NN_Player(model, model, sess, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
