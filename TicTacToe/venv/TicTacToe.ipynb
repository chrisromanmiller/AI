{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermiller/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe\n",
    "import Players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, num_actions, scope, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 9 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "# def update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter):\n",
    "#     old_model = new_model\n",
    "#     old_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter-1))\n",
    "#     \n",
    "#     new_model = TicTacToe_model(board_placeholder, 9, scope = \"model\" + str(update_iter))\n",
    "#     \n",
    "#     new_model_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter))\n",
    "#     sess.run(tf.variables_initializer(new_model_vars))\n",
    "#     \n",
    "#     for var_old, var_new in zip(old_model_vars, new_model_vars):\n",
    "#         var_new = sess.run(var_old)\n",
    "# #        print(sess.run(var_old))\n",
    "# #        print(sess.run(var_new))\n",
    "# #    print(sess.run(old_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "# #    print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#     return old_model, new_model\n",
    "# \n",
    "# def update_and_duplicate_old_model():\n",
    "#     old_model = new_model\n",
    "#     \n",
    "#     variables = tf.trainable_variables()\n",
    "#     for var1 in variables:\n",
    "#         if \"pred/\" in var1.name:\n",
    "#             trained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\n",
    "#             value = sess.run(trained_var)\n",
    "#             sess.run(tf.assign(var1, value))\n",
    "\n",
    "\n",
    "def sample_action(model):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(logits=out)\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "# def sample_trajectory_random_opponent(new_model, sess, env):\n",
    "#     new_sample = sample_action(new_model)\n",
    "#     obs, acs, rewards = [], [], []\n",
    "#     seed = np.random.randint(0,2)\n",
    "#     ob = env.reset()\n",
    "#     if seed % 2 == 0:\n",
    "#         while True:\n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#             \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#     else:\n",
    "#         while True:       \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#    \n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#                 \n",
    "#     path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "#                 \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "#                 \"action\" : np.array(acs, dtype=np.float32)}\n",
    "# #    print(str(len(path[\"observation\"])))\n",
    "#     return path\n",
    "    \n",
    "    \n",
    "def sample_trajectory(opponent, model_input_s, sess, env):\n",
    "    new_sample = model_input_s\n",
    "    # old_sample = sample_action(old_model)\n",
    "    obs, acs, rewards = [], [], []\n",
    "    seed = np.random.randint(0,2)\n",
    "    ob = env.reset()\n",
    "    if seed % 2 == 0:\n",
    "        while True:\n",
    "            obs.append(ob)\n",
    "#            tic = time.time()\n",
    "            player_action = sess.run(new_sample, feed_dict={observation_placeholder: np.array([ob])})\n",
    "#            print(\"player time\", time.time()-tic)\n",
    "            acs.append(player_action)\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "#            tic = time.time()\n",
    "            opponent_action = opponent.policy(np.array([ob]))\n",
    "#            print(\"opponent time\", time.time()-tic)\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "    else:\n",
    "        while True:       \n",
    "            # opponent_action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "#            tic = time.time()\n",
    "            opponent_action = opponent.policy(np.array([ob]))\n",
    "#            print(\"opponent_time\",time.time()-tic)\n",
    "\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "   \n",
    "            obs.append(ob)\n",
    "    \n",
    "#            tic = time.time()\n",
    "            player_action = sess.run(new_sample, feed_dict={observation_placeholder: np.array([ob])})\n",
    "#            print(\"player time\", time.time()-tic)\n",
    "\n",
    "            acs.append(player_action)\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "#    print(str(len(path[\"observation\"])))\n",
    "    return path\n",
    "\n",
    "# def sample_trajectories_random_opponent(new_model, sess, min_timesteps_per_batch, env):\n",
    "#     paths =[]\n",
    "#     timesteps_this_batch = 0\n",
    "#     while True:\n",
    "#         path = sample_trajectory_random_opponent(new_model, sess, env)\n",
    "#         paths.append(path)\n",
    "#         timesteps_this_batch += len(path['observation'])\n",
    "#         if timesteps_this_batch > min_timesteps_per_batch:\n",
    "#             break\n",
    "#     return paths\n",
    "\n",
    "def sample_trajectories(opponent, model_input_s, sess, min_timesteps_per_batch, env):\n",
    "    paths =[]\n",
    "    timesteps_this_batch = 0\n",
    "    while True:\n",
    "        path = sample_trajectory(opponent, model_input_s, sess, env)\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += len(path['observation'])\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    return paths\n",
    "    \n",
    "def sum_of_rewards(paths, gamma = .95): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder):\n",
    "    logits = model\n",
    "    \n",
    "    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= logits)\n",
    "    return log_prob\n",
    "\n",
    "def loss_and_update_op(log_prob,adv_n):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) \n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.381546\n",
      "iteration number 1\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.23925869\n",
      "iteration number 2\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.22244959\n",
      "iteration number 3\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.19868542\n",
      "iteration number 4\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.17349502\n",
      "iteration number 5\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.13239881\n",
      "iteration number 6\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.12628952\n",
      "iteration number 7\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.09774488\n",
      "iteration number 8\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.06730005\n",
      "iteration number 9\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.020205284\n",
      "iteration number 10\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.020788416\n",
      "iteration number 11\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.010421313\n",
      "iteration number 12\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.0102704\n",
      "iteration number 13\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.017387277\n",
      "iteration number 14\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.019244106\n",
      "iteration number 15\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.014164038\n",
      "iteration number 16\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.016439572\n",
      "iteration number 17\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.0023730262\n",
      "iteration number 18\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "-0.0056005144\n",
      "iteration number 19\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.031580936\n",
      "iteration number 20\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.07401943\n",
      "iteration number 21\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.08475988\n",
      "iteration number 22\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.107085265\n",
      "iteration number 23\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.12495682\n",
      "iteration number 24\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.11120872\n",
      "iteration number 25\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.13536645\n",
      "iteration number 26\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.21962148\n",
      "iteration number 27\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.15327612\n",
      "iteration number 28\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.23878288\n",
      "iteration number 29\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.34454483\n",
      "iteration number 30\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.4326098\n",
      "iteration number 31\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.494241\n",
      "iteration number 32\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.5382092\n",
      "iteration number 33\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.5607418\n",
      "iteration number 34\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.5923528\n",
      "iteration number 35\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.6311293\n",
      "iteration number 36\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.59009194\n",
      "iteration number 37\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.62114656\n",
      "iteration number 38\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.63338655\n",
      "iteration number 39\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.5982415\n",
      "iteration number 40\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.5952072\n",
      "iteration number 41\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.70544046\n",
      "iteration number 42\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.6348412\n",
      "iteration number 43\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.65721494\n",
      "iteration number 44\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.6637547\n",
      "iteration number 45\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.65480375\n",
      "iteration number 46\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.6711615\n",
      "iteration number 47\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.8638589\n",
      "iteration number 48\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.38943863\n",
      "iteration number 49\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.3859991\n",
      "iteration number 50\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.40170532\n",
      "iteration number 51\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "0.443329\n",
      "iteration number 52\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "model_input_s = sample_action(model)\n",
    "\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 200\n",
    "number_expert_updates = 100\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "    #Define Loss functions *symbolically*\n",
    "    log_prob = get_log_prob(model, action_placeholder)\n",
    "    loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "    #Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "    if k ==0:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "    \n",
    "    temp_reward = []\n",
    "    \n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        #Produce some trajectories\n",
    "#        print(\"sampling games...\")\n",
    "        paths = sample_trajectories(opponent, model_input_s, sess, 100, env = env)\n",
    "#        print(\"games sampled.\")\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "#        print(\"Last game played\", paths)\n",
    "        \n",
    "        #Update the model\n",
    "#        print(\"updating model...\")\n",
    "#        print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#         test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) #, scope=\"model\" +str(k-1))\n",
    "#         for var in test_vars:\n",
    "#             print(var.name, np.max(sess.run(var)))\n",
    "            \n",
    "#        for i, var in enumerate(test_vars):\n",
    "#            print(var.name, sess.run(var))\n",
    "#        print(\"number of games:\", len(paths))\n",
    "        temp_reward.append(np.mean(np.concatenate([path[\"reward\"] for path in paths])))\n",
    "    \n",
    "\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "#        print(\"model updated.\")\n",
    "    #if new_model does better...\n",
    "    #old_model, model = update_old_model_initialize_new_model(old_model, model, board_placeholder, sess, update_iter= k)\n",
    "    opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder)\n",
    "    print(np.mean(temp_reward))\n",
    "    \"\"\" TODO: UPDATE OPPONENT\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths[0]['observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(a.name, sess.run(a)) for i, a  in enumerate(tf.trainable_variables())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[16].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0\n",
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0049994704\n",
      "model-1/dense/kernel:0 1.5649351e-07\n",
      "model-1/dense_1/kernel:0 0.009812419\n",
      "model-1/dense/kernel:0 1.3215144e-07\n",
      "model-1/dense_1/kernel:0 0.014569419\n",
      "model-1/dense/kernel:0 1.2292737e-07\n",
      "model-1/dense_1/kernel:0 0.018785303\n",
      "model-1/dense/kernel:0 1.5019947e-07\n",
      "model-1/dense_1/kernel:0 0.022343298\n",
      "model-1/dense/kernel:0 3.4073742e-07\n",
      "model-1/dense_1/kernel:0 0.025119584\n",
      "model-1/dense/kernel:0 5.161538e-07\n",
      "model-1/dense_1/kernel:0 0.027365364\n",
      "model-1/dense/kernel:0 6.7784526e-07\n",
      "model-1/dense_1/kernel:0 0.029040836\n",
      "model-1/dense/kernel:0 6.912954e-07\n",
      "model-1/dense_1/kernel:0 0.02831384\n",
      "model-1/dense/kernel:0 7.729489e-07\n",
      "model-1/dense_1/kernel:0 0.027713807\n",
      "model-1/dense/kernel:0 1.090002e-06\n",
      "model-1/dense_1/kernel:0 0.027298247\n",
      "model-1/dense/kernel:0 1.6268523e-06\n",
      "model-1/dense_1/kernel:0 0.027223863\n",
      "model-1/dense/kernel:0 1.8749504e-06\n",
      "model-1/dense_1/kernel:0 0.026900835\n",
      "model-1/dense/kernel:0 1.9321071e-06\n",
      "model-1/dense_1/kernel:0 0.02668874\n",
      "model-1/dense/kernel:0 2.022239e-06\n",
      "model-1/dense_1/kernel:0 0.026247295\n",
      "model-1/dense/kernel:0 2.216402e-06\n",
      "model-1/dense_1/kernel:0 0.028726596\n",
      "model-1/dense/kernel:0 2.6245382e-06\n",
      "model-1/dense_1/kernel:0 0.032282468\n",
      "model-1/dense/kernel:0 3.4380334e-06\n",
      "model-1/dense_1/kernel:0 0.035922382\n",
      "model-1/dense/kernel:0 4.1265153e-06\n",
      "model-1/dense_1/kernel:0 0.03935676\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(new_model, action_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "        #Produce some trajectories\n",
    "        paths = sample_trajectories_random_opponent(new_model, sess, 20, env = env)\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"model\" +str(-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
