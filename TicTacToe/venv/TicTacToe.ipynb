{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermiller/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, num_actions, scope, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 9 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "# def update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter):\n",
    "#     old_model = new_model\n",
    "#     old_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter-1))\n",
    "#     \n",
    "#     new_model = TicTacToe_model(board_placeholder, 9, scope = \"model\" + str(update_iter))\n",
    "#     \n",
    "#     new_model_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter))\n",
    "#     sess.run(tf.variables_initializer(new_model_vars))\n",
    "#     \n",
    "#     for var_old, var_new in zip(old_model_vars, new_model_vars):\n",
    "#         var_new = sess.run(var_old)\n",
    "# #        print(sess.run(var_old))\n",
    "# #        print(sess.run(var_new))\n",
    "# #    print(sess.run(old_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "# #    print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#     return old_model, new_model\n",
    "# \n",
    "# def update_and_duplicate_old_model():\n",
    "#     old_model = new_model\n",
    "#     \n",
    "#     variables = tf.trainable_variables()\n",
    "#     for var1 in variables:\n",
    "#         if \"pred/\" in var1.name:\n",
    "#             trained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\n",
    "#             value = sess.run(trained_var)\n",
    "#             sess.run(tf.assign(var1, value))\n",
    "\n",
    "\n",
    "def sample_action(model):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(logits=out)\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "# def sample_trajectory_random_opponent(new_model, sess, env):\n",
    "#     new_sample = sample_action(new_model)\n",
    "#     obs, acs, rewards = [], [], []\n",
    "#     seed = np.random.randint(0,2)\n",
    "#     ob = env.reset()\n",
    "#     if seed % 2 == 0:\n",
    "#         while True:\n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#             \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#     else:\n",
    "#         while True:       \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#    \n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#                 \n",
    "#     path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "#                 \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "#                 \"action\" : np.array(acs, dtype=np.float32)}\n",
    "# #    print(str(len(path[\"observation\"])))\n",
    "#     return path\n",
    "    \n",
    "    \n",
    "def sample_trajectory(opponent, new_model, sess, env):\n",
    "    new_sample = sample_action(new_model)\n",
    "    # old_sample = sample_action(old_model)\n",
    "    obs, acs, rewards = [], [], []\n",
    "    seed = np.random.randint(0,2)\n",
    "    ob = env.reset()\n",
    "    if seed % 2 == 0:\n",
    "        while True:\n",
    "            obs.append(ob)\n",
    "            player_action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "            acs.append(player_action)\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "            opponent_action = opponent.policy([ob])\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "    else:\n",
    "        while True:       \n",
    "            # opponent_action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "            opponent_action = opponent.policy([ob])\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "   \n",
    "            obs.append(ob)\n",
    "            player_action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "            acs.append(player_action)\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "#    print(str(len(path[\"observation\"])))\n",
    "    return path\n",
    "\n",
    "# def sample_trajectories_random_opponent(new_model, sess, min_timesteps_per_batch, env):\n",
    "#     paths =[]\n",
    "#     timesteps_this_batch = 0\n",
    "#     while True:\n",
    "#         path = sample_trajectory_random_opponent(new_model, sess, env)\n",
    "#         paths.append(path)\n",
    "#         timesteps_this_batch += len(path['observation'])\n",
    "#         if timesteps_this_batch > min_timesteps_per_batch:\n",
    "#             break\n",
    "#     return paths\n",
    "\n",
    "def sample_trajectories(opponent, new_model, sess, min_timesteps_per_batch, env):\n",
    "    paths =[]\n",
    "    timesteps_this_batch = 0\n",
    "    while True:\n",
    "        path = sample_trajectory(opponent, new_model, sess, env)\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += len(path['observation'])\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    return paths\n",
    "    \n",
    "def sum_of_rewards(paths, gamma = .95): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder):\n",
    "    logits = model\n",
    "    \n",
    "    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= logits)\n",
    "    return log_prob\n",
    "\n",
    "def loss_and_update_op(log_prob,adv_n):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) \n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "model-1/dense/kernel:0 0.28542724\n",
      "model-1/dense/bias:0 0.0\n",
      "model-1/dense_1/kernel:0 0.21645473\n",
      "model-1/dense_1/bias:0 0.0\n",
      "model-1/dense_2/kernel:0 0.0\n",
      "model-1/dense_2/bias:0 0.0\n",
      "model-1/dense/kernel:0 0.28542724\n",
      "model-1/dense/bias:0 0.0\n",
      "model-1/dense_1/kernel:0 0.21645473\n",
      "model-1/dense_1/bias:0 0.0\n",
      "model-1/dense_2/kernel:0 0.0049995645\n",
      "model-1/dense_2/bias:0 0.0049999934\n",
      "model-1/dense/kernel:0 0.28542754\n",
      "model-1/dense/bias:0 4.489061e-07\n",
      "model-1/dense_1/kernel:0 0.21645474\n",
      "model-1/dense_1/bias:0 2.2642174e-05\n",
      "model-1/dense_2/kernel:0 0.009904584\n",
      "model-1/dense_2/bias:0 0.009901335\n",
      "model-1/dense/kernel:0 0.28539768\n",
      "model-1/dense/bias:0 4.3357795e-05\n",
      "model-1/dense_1/kernel:0 0.21645452\n",
      "model-1/dense_1/bias:0 0.0009455467\n",
      "model-1/dense_2/kernel:0 0.01408641\n",
      "model-1/dense_2/bias:0 0.014069178\n",
      "model-1/dense/kernel:0 0.28537646\n",
      "model-1/dense/bias:0 9.6450494e-05\n",
      "model-1/dense_1/kernel:0 0.2164719\n",
      "model-1/dense_1/bias:0 0.0021572076\n",
      "model-1/dense_2/kernel:0 0.018579526\n",
      "model-1/dense_2/bias:0 0.018552288\n",
      "model-1/dense/kernel:0 0.28573954\n",
      "model-1/dense/bias:0 0.00043173658\n",
      "model-1/dense_1/kernel:0 0.21654256\n",
      "model-1/dense_1/bias:0 0.0045234226\n",
      "model-1/dense_2/kernel:0 0.019023957\n",
      "model-1/dense_2/bias:0 0.018971518\n",
      "model-1/dense/kernel:0 0.28607002\n",
      "model-1/dense/bias:0 0.00080736517\n",
      "model-1/dense_1/kernel:0 0.2167258\n",
      "model-1/dense_1/bias:0 0.007029842\n",
      "model-1/dense_2/kernel:0 0.021756366\n",
      "model-1/dense_2/bias:0 0.02168167\n",
      "model-1/dense/kernel:0 0.28656492\n",
      "model-1/dense/bias:0 0.0013085194\n",
      "model-1/dense_1/kernel:0 0.21722889\n",
      "model-1/dense_1/bias:0 0.010164987\n",
      "model-1/dense_2/kernel:0 0.023145646\n",
      "model-1/dense_2/bias:0 0.023064764\n",
      "model-1/dense/kernel:0 0.28703701\n",
      "model-1/dense/bias:0 0.001724935\n",
      "model-1/dense_1/kernel:0 0.21770315\n",
      "model-1/dense_1/bias:0 0.01234982\n",
      "model-1/dense_2/kernel:0 0.024066247\n",
      "model-1/dense_2/bias:0 0.0239958\n",
      "model-1/dense/kernel:0 0.28745186\n",
      "model-1/dense/bias:0 0.0021362714\n",
      "model-1/dense_1/kernel:0 0.21799801\n",
      "model-1/dense_1/bias:0 0.013835055\n",
      "model-1/dense_2/kernel:0 0.025526442\n",
      "model-1/dense_2/bias:0 0.025439646\n",
      "iteration number 1\n",
      "model0/dense/kernel:0 0.28598896\n",
      "model0/dense/bias:0 0.0\n",
      "model0/dense_1/kernel:0 0.21647997\n",
      "model0/dense_1/bias:0 0.0\n",
      "model0/dense_2/kernel:0 0.0\n",
      "model0/dense_2/bias:0 0.0\n",
      "model0/dense/kernel:0 0.28598896\n",
      "model0/dense/bias:0 0.0\n",
      "model0/dense_1/kernel:0 0.21647997\n",
      "model0/dense_1/bias:0 0.0\n",
      "model0/dense_2/kernel:0 0.0049996483\n",
      "model0/dense_2/bias:0 0.004999994\n",
      "model0/dense/kernel:0 0.28598902\n",
      "model0/dense/bias:0 3.2835416e-08\n",
      "model0/dense_1/kernel:0 0.21647996\n",
      "model0/dense_1/bias:0 1.5406526e-06\n",
      "model0/dense_2/kernel:0 0.00989343\n",
      "model0/dense_2/bias:0 0.009885421\n",
      "model0/dense/kernel:0 0.28592062\n",
      "model0/dense/bias:0 0.0005557072\n",
      "model0/dense_1/kernel:0 0.21655647\n",
      "model0/dense_1/bias:0 0.0027133373\n",
      "model0/dense_2/kernel:0 0.014594156\n",
      "model0/dense_2/bias:0 0.014559289\n",
      "model0/dense/kernel:0 0.28581527\n",
      "model0/dense/bias:0 0.0010304688\n",
      "model0/dense_1/kernel:0 0.21664451\n",
      "model0/dense_1/bias:0 0.0047534388\n",
      "model0/dense_2/kernel:0 0.017277585\n",
      "model0/dense_2/bias:0 0.017234927\n",
      "model0/dense/kernel:0 0.28573552\n",
      "model0/dense/bias:0 0.0016969595\n",
      "model0/dense_1/kernel:0 0.21681674\n",
      "model0/dense_1/bias:0 0.007066522\n",
      "model0/dense_2/kernel:0 0.020836053\n",
      "model0/dense_2/bias:0 0.020777872\n",
      "model0/dense/kernel:0 0.28620663\n",
      "model0/dense/bias:0 0.0023358648\n",
      "model0/dense_1/kernel:0 0.216978\n",
      "model0/dense_1/bias:0 0.009711768\n",
      "model0/dense_2/kernel:0 0.022905339\n",
      "model0/dense_2/bias:0 0.022820892\n",
      "model0/dense/kernel:0 0.2866207\n",
      "model0/dense/bias:0 0.0026406117\n",
      "model0/dense_1/kernel:0 0.21696131\n",
      "model0/dense_1/bias:0 0.011089771\n",
      "model0/dense_2/kernel:0 0.024670644\n",
      "model0/dense_2/bias:0 0.024565758\n",
      "model0/dense/kernel:0 0.2869005\n",
      "model0/dense/bias:0 0.0028273258\n",
      "model0/dense_1/kernel:0 0.21690226\n",
      "model0/dense_1/bias:0 0.011537102\n",
      "model0/dense_2/kernel:0 0.026396882\n",
      "model0/dense_2/bias:0 0.026273139\n",
      "model0/dense/kernel:0 0.2865011\n",
      "model0/dense/bias:0 0.0022222982\n",
      "model0/dense_1/kernel:0 0.21710452\n",
      "model0/dense_1/bias:0 0.010463483\n",
      "model0/dense_2/kernel:0 0.026718989\n",
      "model0/dense_2/bias:0 0.026570175\n",
      "iteration number 2\n",
      "model1/dense/kernel:0 0.28587\n",
      "model1/dense/bias:0 0.0\n",
      "model1/dense_1/kernel:0 0.21639733\n",
      "model1/dense_1/bias:0 0.0\n",
      "model1/dense_2/kernel:0 0.0\n",
      "model1/dense_2/bias:0 0.0\n",
      "model1/dense/kernel:0 0.28587\n",
      "model1/dense/bias:0 0.0\n",
      "model1/dense_1/kernel:0 0.21639733\n",
      "model1/dense_1/bias:0 0.0\n",
      "model1/dense_2/kernel:0 0.004999754\n",
      "model1/dense_2/bias:0 0.0049999957\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f64cd0c3abe3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#Produce some trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#        print(\"sampling games...\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m#        print(\"games sampled.\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_of_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a4bc0bab0e8b>\u001b[0m in \u001b[0;36msample_trajectories\u001b[0;34m(old_model, new_model, sess, min_timesteps_per_batch, env)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a4bc0bab0e8b>\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(old_model, new_model, sess, env)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mnew_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m     \u001b[0mold_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a4bc0bab0e8b>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/distributions/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, logits, probs, dtype, validate_args, allow_nan_stats, name)\u001b[0m\n\u001b[1;32m    222\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batch_shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_shape_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m     super(Categorical, self).__init__(\n\u001b[1;32m    226\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m       packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n\u001b[0;32m--> 500\u001b[0;31m                                                   stack(strides))\n\u001b[0m\u001b[1;32m    501\u001b[0m       if (packed_begin.dtype == dtypes.int64 or\n\u001b[1;32m    502\u001b[0m           \u001b[0mpacked_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m       \u001b[0;31m# If the input is a constant list, it can be converted to a constant op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Input list contains non-constant tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    215\u001b[0m                                          as_ref=False):\n\u001b[1;32m    216\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    200\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[1;32m    201\u001b[0m              \"dtype\": dtype_value},\n\u001b[0;32m--> 202\u001b[0;31m       name=name).outputs[0]\n\u001b[0m\u001b[1;32m    203\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 instructions)\n\u001b[0;32m--> 454\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    456\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3153\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3155\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3156\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1729\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1730\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1731\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m   op_desc = c_api.TF_NewOperation(graph._c_graph,\n\u001b[1;32m   1554\u001b[0m                                   \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1555\u001b[0;31m                                   compat.as_str(node_def.name))\n\u001b[0m\u001b[1;32m   1556\u001b[0m   \u001b[0;31m# Add inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1557\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mop_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "opponent = Players.RandomPlayer()\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "    #Define Loss functions *symbolically*\n",
    "    log_prob = get_log_prob(model, action_placeholder)\n",
    "    loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "    #Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "    if k ==0:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "    \n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        #Produce some trajectories\n",
    "#        print(\"sampling games...\")\n",
    "        paths = sample_trajectories(opponent, model, sess, 20, env = env)\n",
    "#        print(\"games sampled.\")\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "#        print(\"Last game played\", paths)\n",
    "        \n",
    "        #Update the model\n",
    "#        print(\"updating model...\")\n",
    "#        print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) #, scope=\"model\" +str(k-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "            \n",
    "#        for i, var in enumerate(test_vars):\n",
    "#            print(var.name, sess.run(var))\n",
    "\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "#        print(\"model updated.\")\n",
    "    #if new_model does better...\n",
    "    #old_model, model = update_old_model_initialize_new_model(old_model, model, board_placeholder, sess, update_iter= k)\n",
    "    opponent = Players.NN_Player(model, sess, observation_placeholder)\n",
    "    \"\"\" TODO: UPDATE OPPONENT\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "for path in paths:\n",
    "    print(path['observation'])\n",
    "    print(path['action'])\n",
    "    print(path['reward'])\n",
    "    print(\"____\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths[0]['observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(a.name, sess.run(a)) for i, a  in enumerate(tf.trainable_variables())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[16].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0\n",
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0049994704\n",
      "model-1/dense/kernel:0 1.5649351e-07\n",
      "model-1/dense_1/kernel:0 0.009812419\n",
      "model-1/dense/kernel:0 1.3215144e-07\n",
      "model-1/dense_1/kernel:0 0.014569419\n",
      "model-1/dense/kernel:0 1.2292737e-07\n",
      "model-1/dense_1/kernel:0 0.018785303\n",
      "model-1/dense/kernel:0 1.5019947e-07\n",
      "model-1/dense_1/kernel:0 0.022343298\n",
      "model-1/dense/kernel:0 3.4073742e-07\n",
      "model-1/dense_1/kernel:0 0.025119584\n",
      "model-1/dense/kernel:0 5.161538e-07\n",
      "model-1/dense_1/kernel:0 0.027365364\n",
      "model-1/dense/kernel:0 6.7784526e-07\n",
      "model-1/dense_1/kernel:0 0.029040836\n",
      "model-1/dense/kernel:0 6.912954e-07\n",
      "model-1/dense_1/kernel:0 0.02831384\n",
      "model-1/dense/kernel:0 7.729489e-07\n",
      "model-1/dense_1/kernel:0 0.027713807\n",
      "model-1/dense/kernel:0 1.090002e-06\n",
      "model-1/dense_1/kernel:0 0.027298247\n",
      "model-1/dense/kernel:0 1.6268523e-06\n",
      "model-1/dense_1/kernel:0 0.027223863\n",
      "model-1/dense/kernel:0 1.8749504e-06\n",
      "model-1/dense_1/kernel:0 0.026900835\n",
      "model-1/dense/kernel:0 1.9321071e-06\n",
      "model-1/dense_1/kernel:0 0.02668874\n",
      "model-1/dense/kernel:0 2.022239e-06\n",
      "model-1/dense_1/kernel:0 0.026247295\n",
      "model-1/dense/kernel:0 2.216402e-06\n",
      "model-1/dense_1/kernel:0 0.028726596\n",
      "model-1/dense/kernel:0 2.6245382e-06\n",
      "model-1/dense_1/kernel:0 0.032282468\n",
      "model-1/dense/kernel:0 3.4380334e-06\n",
      "model-1/dense_1/kernel:0 0.035922382\n",
      "model-1/dense/kernel:0 4.1265153e-06\n",
      "model-1/dense_1/kernel:0 0.03935676\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(new_model, action_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "        #Produce some trajectories\n",
    "        paths = sample_trajectories_random_opponent(new_model, sess, 20, env = env)\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"model\" +str(-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154176\n",
      "93104\n"
     ]
    }
   ],
   "source": [
    "width_low_tower  = 24\n",
    "height_low_tower = 77\n",
    "\n",
    "n = width_low_tower\n",
    "k = height_low_tower\n",
    "\n",
    "withinLevels  = 0\n",
    "betweenLevels = 0\n",
    "for j in range(1,n):\n",
    "    withinLevels += 2*j*(j-1)\n",
    "    betweenLevels += 4*j*j\n",
    "    \n",
    "pyramid = withinLevels + betweenLevels\n",
    "\n",
    "lower_tower_within = 2*k*n*(n-1)\n",
    "lower_tower_between = (k-1)*n*n\n",
    "\n",
    "lower_tower = lower_tower_within + lower_tower_between\n",
    "\n",
    "    \n",
    "print(pyramid+lower_tower)\n",
    "print(withinLevels + lower_tower_within)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
