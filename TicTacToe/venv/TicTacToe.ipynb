{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe\n",
    "import Players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, num_actions, scope, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 9 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "# def update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter):\n",
    "#     old_model = new_model\n",
    "#     old_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter-1))\n",
    "#     \n",
    "#     new_model = TicTacToe_model(board_placeholder, 9, scope = \"model\" + str(update_iter))\n",
    "#     \n",
    "#     new_model_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter))\n",
    "#     sess.run(tf.variables_initializer(new_model_vars))\n",
    "#     \n",
    "#     for var_old, var_new in zip(old_model_vars, new_model_vars):\n",
    "#         var_new = sess.run(var_old)\n",
    "# #        print(sess.run(var_old))\n",
    "# #        print(sess.run(var_new))\n",
    "# #    print(sess.run(old_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "# #    print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#     return old_model, new_model\n",
    "# \n",
    "# def update_and_duplicate_old_model():\n",
    "#     old_model = new_model\n",
    "#     \n",
    "#     variables = tf.trainable_variables()\n",
    "#     for var1 in variables:\n",
    "#         if \"pred/\" in var1.name:\n",
    "#             trained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\n",
    "#             value = sess.run(trained_var)\n",
    "#             sess.run(tf.assign(var1, value))\n",
    "\n",
    "\n",
    "def sample_action(model, mask_placeholder):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(probs=maskedSoftmax(out, mask_placeholder))\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "\"\"\"Code from online\"\"\"\n",
    "def maskedSoftmax(logits, mask):\n",
    "    \"\"\"\n",
    "    Masked softmax over dim 1\n",
    "    :param logits: (N, L)\n",
    "    :param mask: (N, L)\n",
    "    :return: probabilities (N, L)\n",
    "    \"\"\"\n",
    "    indices = tf.where(mask)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    result = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "    result.set_shape(logits.shape)\n",
    "    return result\n",
    "\n",
    "# def sample_trajectory_random_opponent(new_model, sess, env):\n",
    "#     new_sample = sample_action(new_model)\n",
    "#     obs, acs, rewards = [], [], []\n",
    "#     seed = np.random.randint(0,2)\n",
    "#     ob = env.reset()\n",
    "#     if seed % 2 == 0:\n",
    "#         while True:\n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#             \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#     else:\n",
    "#         while True:       \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#    \n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#                 \n",
    "#     path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "#                 \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "#                 \"action\" : np.array(acs, dtype=np.float32)}\n",
    "# #    print(str(len(path[\"observation\"])))\n",
    "#     return path\n",
    "    \n",
    "\n",
    "def sample_trajectory(player, opponent, env):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards = [], [], []\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        if env.current_player == 1:\n",
    "            obs.append(ob)\n",
    "            player_action = player.policy(np.array([ob]))\n",
    "            acs.append(player_action[0])\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        else:\n",
    "            opponent_action = opponent.policy(np.array([ob]))\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "    return path\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# def sample_trajectories(player, opponent, env, min_timesteps_per_batch):\n",
    "#     paths =[]\n",
    "#     timesteps_this_batch = 0\n",
    "#     while True:\n",
    "#         path = sample_trajectory(player, opponent, env)\n",
    "#         paths.append(path)\n",
    "#         timesteps_this_batch += len(path['observation'])\n",
    "#         if timesteps_this_batch > min_timesteps_per_batch:\n",
    "#             break\n",
    "#     return paths\n",
    "    \n",
    "def sum_of_rewards(paths, gamma = 1): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder, mask_placeholder):\n",
    "    logits = model\n",
    "    \n",
    "    indices = tf.where(mask_placeholder)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    result = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "    result.set_shape(logits.shape)\n",
    "    \n",
    "    \"\"\"Want to emulate this:\"\"\"\n",
    "    log_prob = nn.softmax(logits)* \n",
    "#    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= tf.SparseTensor(indices, values, denseShape))\n",
    "    return log_prob\n",
    "\n",
    "def loss_and_update_op(log_prob,adv_n):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) \n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"Where_1:0\", shape=(?, 2), dtype=int64), values=Tensor(\"GatherNd_1:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"Cast_1:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m       \u001b[0mstr_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     60\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[0;32m---> 61\u001b[0;31m                     (bytes_or_text,))\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected binary or unicode string, got <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x1c2433b4a8>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c72921a6b502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#Define Loss functions *symbolically*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_and_update_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_n_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3585b8cd1054>\u001b[0m in \u001b[0;36mget_log_prob\u001b[0;34m(model, action_placeholder, mask_placeholder)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mdenseShape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m     \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0maction_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenseShape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   2033\u001b[0m                       [labels, logits]):\n\u001b[1;32m   2034\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2035\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2036\u001b[0m     precise_logits = math_ops.cast(logits, dtypes.float32) if (dtypes.as_dtype(\n\u001b[1;32m   2037\u001b[0m         logits.dtype) == dtypes.float16) else logits\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    996\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    215\u001b[0m                                          as_ref=False):\n\u001b[1;32m    216\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    194\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    195\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 196\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    197\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    523\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[1;32m    524\u001b[0m                       \u001b[0;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[1;32m    526\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(\"Where_1:0\", shape=(?, 2), dtype=int64), values=Tensor(\"GatherNd_1:0\", shape=(?,), dtype=float32), dense_shape=Tensor(\"Cast_1:0\", shape=(2,), dtype=int64)). Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models *symbolically*\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "mask_placeholder = tf.placeholder(shape=[None, 9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "model_input_s = sample_action(model, mask_placeholder)\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(model, action_placeholder, mask_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#Defines player, opponent\n",
    "player = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder, duplicate=False)\n",
    "opponent = Players.Random_Player()\n",
    "opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder)\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 200\n",
    "number_expert_updates = 100\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "\n",
    "    #Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "#     if k ==0:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#     else:\n",
    "#         sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "    \n",
    "    batch_adv_n = []\n",
    "    batch_illegal = []\n",
    "    batch_wins = []\n",
    "    \n",
    "    tic = time.time()\n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        #Produce some trajectories\n",
    "#        print(\"sampling games...\")\n",
    "        max_time_step = 100\n",
    "        time_step = 0\n",
    "        paths = []\n",
    "        while time_step < max_time_step:\n",
    "            path = sample_trajectory(player,opponent,env)\n",
    "            paths.append(path)\n",
    "            time_step += len(path['reward'])\n",
    "            batch_wins.append(env.current_winner)\n",
    "            batch_illegal.append(env.illegal_moves_N[0])\n",
    "#         paths = sample_trajectories(player, opponent, env, 100)\n",
    "\n",
    "        adv_n = sum_of_rewards(paths)\n",
    "        batch_adv_n = batch_adv_n + adv_n\n",
    "        \n",
    "\n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "\n",
    "\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "\n",
    "    \n",
    "    #Unwind win data:\n",
    "    batch_win_unique, batch_win_counts = np.unique(batch_wins, return_counts=True)\n",
    "    batch_win_percentage = dict(zip(batch_win_unique, batch_win_counts))[1]*1.0/len(batch_wins)\n",
    "    if batch_win_percentage > .6:\n",
    "        opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder)\n",
    "            \n",
    "    print(\"mean adv\", np.mean(batch_adv_n))\n",
    "    print(\"mean illegal\", np.mean(batch_illegal))\n",
    "    print(\"win percentage\", batch_win_percentage)\n",
    "    print(\"iteration time\", time.time() - tic)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./bot_10_28_v2.ckpt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_file_name = './bot_10_28_v2.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(a.name, sess.run(a)) for i, a  in enumerate(tf.trainable_variables())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[16].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0\n",
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0049994704\n",
      "model-1/dense/kernel:0 1.5649351e-07\n",
      "model-1/dense_1/kernel:0 0.009812419\n",
      "model-1/dense/kernel:0 1.3215144e-07\n",
      "model-1/dense_1/kernel:0 0.014569419\n",
      "model-1/dense/kernel:0 1.2292737e-07\n",
      "model-1/dense_1/kernel:0 0.018785303\n",
      "model-1/dense/kernel:0 1.5019947e-07\n",
      "model-1/dense_1/kernel:0 0.022343298\n",
      "model-1/dense/kernel:0 3.4073742e-07\n",
      "model-1/dense_1/kernel:0 0.025119584\n",
      "model-1/dense/kernel:0 5.161538e-07\n",
      "model-1/dense_1/kernel:0 0.027365364\n",
      "model-1/dense/kernel:0 6.7784526e-07\n",
      "model-1/dense_1/kernel:0 0.029040836\n",
      "model-1/dense/kernel:0 6.912954e-07\n",
      "model-1/dense_1/kernel:0 0.02831384\n",
      "model-1/dense/kernel:0 7.729489e-07\n",
      "model-1/dense_1/kernel:0 0.027713807\n",
      "model-1/dense/kernel:0 1.090002e-06\n",
      "model-1/dense_1/kernel:0 0.027298247\n",
      "model-1/dense/kernel:0 1.6268523e-06\n",
      "model-1/dense_1/kernel:0 0.027223863\n",
      "model-1/dense/kernel:0 1.8749504e-06\n",
      "model-1/dense_1/kernel:0 0.026900835\n",
      "model-1/dense/kernel:0 1.9321071e-06\n",
      "model-1/dense_1/kernel:0 0.02668874\n",
      "model-1/dense/kernel:0 2.022239e-06\n",
      "model-1/dense_1/kernel:0 0.026247295\n",
      "model-1/dense/kernel:0 2.216402e-06\n",
      "model-1/dense_1/kernel:0 0.028726596\n",
      "model-1/dense/kernel:0 2.6245382e-06\n",
      "model-1/dense_1/kernel:0 0.032282468\n",
      "model-1/dense/kernel:0 3.4380334e-06\n",
      "model-1/dense_1/kernel:0 0.035922382\n",
      "model-1/dense/kernel:0 4.1265153e-06\n",
      "model-1/dense_1/kernel:0 0.03935676\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(new_model, action_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "        #Produce some trajectories\n",
    "        paths = sample_trajectories_random_opponent(new_model, sess, 20, env = env)\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"model\" +str(-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_trajectory(player, opponent, env):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards = [], [], []\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        if env.current_player = 1:\n",
    "            obs.append(ob)\n",
    "            player_action = player.policy(np.array([ob]))\n",
    "            acs.append(player_action[0])\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        else:\n",
    "            opponent_action = opponent.policy(np.array([ob]))\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "    return path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
