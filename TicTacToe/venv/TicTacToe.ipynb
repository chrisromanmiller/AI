{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'TicTacToe' from '/home/nick/AI/TicTacToe/venv/TicTacToe.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe\n",
    "from collections import Counter\n",
    "import Players\n",
    "from importlib import reload\n",
    "reload(Players)\n",
    "reload(TicTacToe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, scope, reuse=tf.AUTO_REUSE, num_actions = 9):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, num_actions , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "def sample_action(model, mask_placeholder):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(probs=maskedSoftmax(out, mask_placeholder))\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "\"\"\"Code from online\"\"\"\n",
    "def maskedSoftmax(logits, mask):\n",
    "    \"\"\"\n",
    "    Masked softmax over dim 1\n",
    "    :param logits: (N, L)\n",
    "    :param mask: (N, L)\n",
    "    :return: probabilities (N, L)\n",
    "    \"\"\"\n",
    "    indices = tf.where(mask)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    result = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "    result.set_shape(logits.shape)\n",
    "    return result\n",
    "\n",
    "\n",
    "def batch_rollout(player,opponent, env, max_time_steps = 100, exploration_on =False, epsilon =.1):\n",
    "    paths = []\n",
    "    batch_winners = Counter({0: 0, 1: 0, 2:0})\n",
    "    time_steps = 0\n",
    "    while time_steps < max_time_steps:\n",
    "        path = sample_trajectory(player,opponent,env, exploration_on, epsilon)\n",
    "        paths += [path]\n",
    "        batch_winners[env.current_winner] +=1\n",
    "        time_steps += len(path['observation'])\n",
    "    return paths, batch_winners\n",
    "    \n",
    "    \n",
    "    \n",
    "def sample_trajectory(player, opponent, env, exploration_on = False, epsilon = .1):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards, masks = [], [], [], []\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    player_has_acted = False\n",
    "    action = None\n",
    "    \n",
    "    #Do rest of moves\n",
    "    while not done:\n",
    "        #Get current observation of current player\n",
    "        ob = env.get_observation(env.current_player)\n",
    "        legal_moves = env.legal_moves()\n",
    "        if env.current_player == 1:\n",
    "            #Reward is recorded as results of state,action pair... need to check player 1 has acted already\n",
    "            if player_has_acted:\n",
    "                rewards.append(env.get_reward(1))\n",
    "            else:\n",
    "                player_has_acted = True\n",
    "                \n",
    "            action = player.policy(np.array([ob]), np.array([legal_moves]))\n",
    "            if exploration_on:\n",
    "                legal_options = np.array(legal_moves)\n",
    "                action = [random.choice(np.nonzero(legal_options)[0])]\n",
    "            obs.append(ob)\n",
    "            acs.append(action[0])\n",
    "            masks.append(legal_moves)\n",
    "        else:\n",
    "            action = opponent.policy(np.array([ob]), np.array([legal_moves]))\n",
    "        done, _ = env.step(action[0]) \n",
    "\n",
    "    #Need to record final reward for player 1\n",
    "    rewards.append(env.get_reward(1))\n",
    "    \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.int32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.int32),\n",
    "                \"mask\" : np.array(masks, dtype=np.int32)}\n",
    "    return path\n",
    "\n",
    "    \n",
    "    \n",
    "def sum_of_rewards(paths, gamma = .6): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder, mask_placeholder):\n",
    "    action_dim = 9 \n",
    "    logits = model\n",
    "    \n",
    "    indices = tf.where(mask_placeholder)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    probability_dist = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "#     probability_dist = probability_dist.set_shape(logits.shape)\n",
    "    log_probability_dist = tf.scatter_nd(sparseResult.indices, tf.log(sparseResult.values), sparseResult.dense_shape)\n",
    "\n",
    "    \"\"\"Want to emulate this:\"\"\"\n",
    "#     probability_dist = tf.nn.softmax(logits)\n",
    "#     legal_pseudo_probability_dist = probability_dist*values\n",
    "#     legalprobability_dist = tf.divide(legal_pseudo_probability_dist, tf.reduce_sum(legal_pseudo_probability_dist, axis= 1))\n",
    "    \n",
    "    prod = tf.multiply(probability_dist, tf.one_hot(action_placeholder, action_dim ))\n",
    "    \n",
    "    entropy = - tf.reduce_sum(probability_dist * log_probability_dist, axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_prob = tf.log(tf.reduce_sum(prod , axis = 1 ))\n",
    "#    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= tf.SparseTensor(indices, values, denseShape))\n",
    "    return log_prob, entropy\n",
    "\n",
    "def loss_and_update_op(log_prob, entropy, adv_n, entropy_coeff = .1):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) -  entropy_coeff * entropy\n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "iteration number 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c5d94b2a777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0miteration_winners\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_winners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_of_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mbatch_adv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_adv_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madv_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ff256ebc2ce1>\u001b[0m in \u001b[0;36msum_of_rewards\u001b[0;34m(paths, gamma)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mweighted_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mq_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madv_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1868\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1869\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main code for running policy gradient\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models *symbolically*\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "mask_placeholder = tf.placeholder(shape=[None, 9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"policy_gradient\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "model_input_s = sample_action(model, mask_placeholder)\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob, entropy = get_log_prob(model, action_placeholder, mask_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, entropy, adv_n_placeholder, entropy_coeff = 0)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#Defines player, opponent\n",
    "player = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder, duplicate=False)\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "#Loads old player,opponent\n",
    "# temp_file_name = './bot_10_28_v6.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, temp_file_name)\n",
    "\n",
    "\n",
    "# opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 5\n",
    "number_expert_updates = 1000\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "    \n",
    "    batch_adv_n = []\n",
    "    iteration_winners = Counter({0:0,1:0,2:0})\n",
    "    \n",
    "    tic = time.time()\n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        paths, batch_winners = batch_rollout(player, opponent, env, max_time_steps=1000)\n",
    "        iteration_winners += batch_winners\n",
    "        \n",
    "        adv_n = sum_of_rewards(paths)\n",
    "        batch_adv_n = batch_adv_n + adv_n\n",
    "        \n",
    "\n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        masks = np.concatenate([path['mask'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        \n",
    "        sess.run(update_op, feed_dict = {mask_placeholder: masks, adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "    \n",
    "    \n",
    "    #Unwind win data:\n",
    "#     print(iteration_winners)\n",
    "    print(\"mean adv\", np.mean(batch_adv_n))\n",
    "    print(\"iteration time\", time.time() - tic)\n",
    "#     print(paths[0])\n",
    "    \n",
    "    \n",
    "    expert_player = Players.Expert_Player()\n",
    "    _, expert_batch_winners = batch_rollout(player, expert_player, env, max_time_steps=900, exploration_on =True, )\n",
    "    player_loss_percentage_vs_expert = expert_batch_winners[2]*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "    print(\"loss percent vs expert\", player_loss_percentage_vs_expert)\n",
    "    opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder,mask_placeholder)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./bot_10_28_v6.ckpt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save current net\n",
    "\n",
    "temp_file_name = './bot_10_28_v7.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bot_10_28_v3.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Load current net\n",
    "\n",
    "temp_file_name = './bot_10_28_v3.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 867, 1: 0, 2: 9133}\n"
     ]
    }
   ],
   "source": [
    "#Cell Tests Players against each other\n",
    "\n",
    "from importlib import reload\n",
    "reload(Players)\n",
    "player = Players.Random_Player()\n",
    "opponent = Players.Expert_Player()\n",
    "\n",
    "env = TicTacToe.TicTacToe()\n",
    "batch_winners = {0:0,1:0,2:0}\n",
    "for i in range(10000):\n",
    "    path = sample_trajectory(player,opponent,env)\n",
    "    batch_winners[env.current_winner] += 1\n",
    "print(batch_winners)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Q-learning....\n",
    "from importlib import reload \n",
    "reload(Players)\n",
    "\n",
    "def collect_tuples(model, observation_placeholder, opponent, env, sess, num_tuples =100, exploration =.1):\n",
    "    current_state1 = None\n",
    "    current_action = None\n",
    "    current_reward = None\n",
    "    current_state2 = None\n",
    "    tuples = [] #or maybe an array?\n",
    "    ob = env.reset\n",
    "    i=0\n",
    "    while i< num_tuples:\n",
    "        i+=1\n",
    "        #Record the current state\n",
    "        current_state1 = ob\n",
    "\n",
    "        #We pick the best action epsilon-greedily\n",
    "        action = sess.run( model, feed_dict = {observation_placeholder: [ob]})[0] #or something\n",
    "        if np.random.uniform() < exploration:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        #Record the current action and reward\n",
    "        ob = env.step(action)\n",
    "        current_action = action\n",
    "        current_reward = env.get_reward() #or whatever\n",
    "        \n",
    "        #Record the new state\n",
    "        current_state2 = ob\n",
    "        tuples.append({'state1': current_state1, 'action':current_action, 'state2':current_state2, 'reward':current_reward})\n",
    "        \n",
    "    return tuples\n",
    "\n",
    "\"\"\"technically uses hack, assumes all \"\"\"\n",
    "# def arg_max_sample(model, mask_placeholder):\n",
    "#     masked_val = tf.minimum(model,  (2* tf.to_float(mask) - 1) * np.inf))\n",
    "#     masked_arg_max = tf.argmax(masked_val, axis=1)\n",
    "#     return masked_arg_max\n",
    "\n",
    "def get_loss_and_optimizer_Q(model, targets_placeholder, action_placeholder):\n",
    "    q_action_s = model[:,action_placeholder]\n",
    "    diff = tf.subtract(q_action_s, targets_placeholder)\n",
    "    loss = tf.norm(diff)\n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "\n",
    "def compute_target_values(model, next_state, masks, not_end_of_path, reward, decay = .01):\n",
    "    next_state_Qs = sess.run(model, feed_dict= {observation_placeholder: next_state})\n",
    "    future_expected_reward = []\n",
    "    for next_state_Q, mask in zip(next_state_Qs,masks):\n",
    "        indices = np.where(mask)\n",
    "        values = next_state_Q[indices]\n",
    "        future_expected_reward.append(np.max(values))\n",
    "    return reward + decay * not_end_of_path * future_expected_reward\n",
    "\n",
    "def sample_paths(paths, batch_size = 10):\n",
    "    #Make the easy lists\n",
    "    observation_list = np.concatenate([path['observation'] for path in paths])\n",
    "    action_list = np.concatenate([path['action'] for path in paths])\n",
    "    reward_list = np.concatenate([path['reward'] for path in paths])\n",
    "    mask_list = np.concatenate([path['mask'] for path in paths])\n",
    "\n",
    "    #Make the done list\n",
    "    #Returns 0 if at the terminal step\n",
    "    #Returns 1 otherwise.\n",
    "    list_of_ones = [1] * len(observation_list)\n",
    "    partial_sum =0\n",
    "    for path in paths:\n",
    "        partial_sum += len(path['observation'])\n",
    "        list_of_ones[partial_sum-1] = 0\n",
    "    done_list = list_of_ones\n",
    "    \n",
    "    #Get some random indices\n",
    "    indices = np.random.choice(len(observation_list), batch_size) \n",
    "    \n",
    "    #Select randomly chosen entries\n",
    "    state1 = np.array([observation_list[i] for i in indices])\n",
    "    action = np.array([action_list[i] for i in indices])\n",
    "    state2 = np.array([observation_list[(i+1) % len(observation_list)] for i in indices])\n",
    "    reward = np.array([reward_list[i] for i in indices])\n",
    "    mask = np.array([mask_list[(i+1) % len(observation_list)] for i in indices])\n",
    "    done = np.array([done_list[i] for i in indices])\n",
    "    return state1, action, state2 , reward, mask, done\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'strided_slice/stack_2' (op: 'Pack') with input shapes: [], [?].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1625\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1626\u001b[0;31m     \u001b[0mc_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1627\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'strided_slice/stack_2' (op: 'Pack') with input shapes: [], [?].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-55ccd8288e20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTicTacToe_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Q_learn\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# arg_max_sample_s = arg_max_sample(model, mask_placeholder)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mget_loss_and_optimizer_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#Start a session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-d237c1429c00>\u001b[0m in \u001b[0;36mget_loss_and_optimizer_Q\u001b[0;34m(model, targets_placeholder, action_placeholder)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_loss_and_optimizer_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mq_action_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_placeholder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_action_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m    498\u001b[0m                       [tensor] + begin + end + strides) as name:\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m       packed_begin, packed_end, packed_strides = (stack(begin), stack(end),\n\u001b[0m\u001b[1;32m    501\u001b[0m                                                   stack(strides))\n\u001b[1;32m    502\u001b[0m       if (packed_begin.dtype == dtypes.int64 or\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m    872\u001b[0m                                                       expanded_num_dims))\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mpack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m   4687\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4688\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m-> 4689\u001b[0;31m         \"Pack\", values=values, axis=axis, name=name)\n\u001b[0m\u001b[1;32m   4690\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4691\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m                 instructions)\n\u001b[0;32m--> 488\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3270\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3271\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3272\u001b[0;31m           op_def=op_def)\n\u001b[0m\u001b[1;32m   3273\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3274\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[1;32m   1788\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[1;32m   1789\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[0;32m-> 1790\u001b[0;31m                                 control_input_ops)\n\u001b[0m\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;31m# Initialize self._outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m     \u001b[0;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes must be equal rank, but are 0 and 1\n\tFrom merging shape 0 with other shapes. for 'strided_slice/stack_2' (op: 'Pack') with input shapes: [], [?]."
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "reload(Players)\n",
    "#Define any constants\n",
    "gamma = .95\n",
    "\n",
    "#Define the placeholders\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "\n",
    "#target place holder is r(s,a) + \\gamma \\max_a Q(s',a)\n",
    "target_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "# mask_placeholder = tf.placeholder(shape = [None,9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "#Define the model and loss function\n",
    "\n",
    "model = TicTacToe_model(observation_placeholder, scope = \"Q_learn\", reuse=tf.AUTO_REUSE)\n",
    "# arg_max_sample_s = arg_max_sample(model, mask_placeholder)\n",
    "_, update_op, _  = get_loss_and_optimizer_Q(model, target_placeholder, action_placeholder)\n",
    "\n",
    "#Start a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Define the players\n",
    "player = Players.NN_Player(model, model, sess, observation_placeholder)\n",
    "opponent = Players.Random_Player()\n",
    "env = TicTacToe.TicTacToe()\n",
    "#Collect some tuples\n",
    "# database = []\n",
    "# database += collect_tuples(model, observation_placeholder, opponent, env, sess)\n",
    "# \n",
    "# ###Training time\n",
    "# #I have a tuple, state-action-state from somewhere\n",
    "# sampled_tuple = random.choice(database)\n",
    "# \n",
    "# not_updated_Q = sess.run(player.q_func(), feed_dict = {observation_placeholder: [sampled_tuple['state1']]})\n",
    "# \n",
    "# target_Q = not_updated_Q\n",
    "# target_Q[0, sampled_tuple['action']] =  sampled_tuple['reward'] \n",
    "# target_Q[0, sampled_tuple['action']] += tf.reduce_max(sess.run(player.q_function(), feed_dict = {observation_placeholder: [sampled_tuple['state2']]}), 1)[0]\n",
    "\n",
    "\n",
    "#NEED: call batch_rollouts(), feed into chris function\n",
    "paths, _ = batch_rollout(player, opponent, env)\n",
    "states, actions, next_states, rewards, masks, not_end_of_path = sample_paths(paths, batch_size = 10)\n",
    "\n",
    "\n",
    "\n",
    "target_values = compute_target_values(model, next_states, masks, not_end_of_path, rewards)\n",
    "\n",
    "for shit in zip(target_values, states, actions, next_states, rewards, masks, not_end_of_path):\n",
    "    print(shit)\n",
    "\n",
    "print(target_values)\n",
    "# sess.run(update_op, feed_dict= {observation_placeholder : states, action_placeholder : actions, observation_placeholder : states })\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
