{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import multiplayer_tools\n",
    "import TicTacToe\n",
    "import Players\n",
    "import Policy_Gradient\n",
    "import DQN\n",
    "import Actor_Critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "<class 'tictactoe.mnk_game'>\n",
      "iteration number 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "policy() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-e77c1794cee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m#Player and Opponent play games, player follows the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m#Illegal moves are never played\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_winners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiplayer_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m#The winners are recorded for recordkeeping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/multiplayer_tools.py\u001b[0m in \u001b[0;36mbatch_rollout\u001b[0;34m(player, opponent, env, max_time_steps)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbatch_winners\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_winner\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/multiplayer_tools.py\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(player, opponent, env)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Need to record final reward for player 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: policy() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import tictactoe \n",
    "tf.reset_default_graph()\n",
    "reload(Policy_Gradient)\n",
    "reload(DQN)\n",
    "reload(Players)\n",
    "#reload(tictactoe)\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import multiplayer_tools\n",
    "reload(Policy_Gradient)\n",
    "reload(DQN)\n",
    "reload(Players)\n",
    "reload(TicTacToe)\n",
    "\n",
    "\n",
    "\n",
    "#Main code for running policy gradient\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Define the placeholders\n",
    "observation_placeholder = tf.placeholder(shape = [None,2 , 3,3], dtype = tf.int32, name ='observe')\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32, name ='adv_n')\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32, name ='action')\n",
    "mask_placeholder = tf.placeholder(shape=[None, 9], dtype = tf.int32, name= 'mask')\n",
    "target_placeholder = tf.placeholder(shape = [None,], dtype = tf.float32, name = 'target')\n",
    "\n",
    "\n",
    "#Define the model, action distribution\n",
    "model = Policy_Gradient.TicTacToe_model(observation_placeholder, \"actor\", 9)\n",
    "model_input_s = Policy_Gradient.policy_distribution(model)\n",
    "\n",
    "critic = Actor_Critic.TicTacToe_model(observation_placeholder, \"critic\", 1) \n",
    "\n",
    "\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob, entropy = Policy_Gradient.get_log_prob(model, action_placeholder, mask_placeholder)\n",
    "loss, update_op_actor = Policy_Gradient.loss_and_update_op(log_prob, entropy, adv_n_placeholder, entropy_coeff = 0)\n",
    "update_op_critic = DQN.symbolic_Q_update(critic, target_placeholder, action_placeholder)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Defines player, opponent\n",
    "player = Players.NN_Player(model, model_input_s, sess, observation_placeholder, duplicate=False, deterministic = False)\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start an environment\n",
    "env = tictactoe.mnk_game()\n",
    "\n",
    "#Set parameters for update\n",
    "number_target_updates = 1000\n",
    "number_gradient_steps = 5\n",
    "\n",
    "\n",
    "\n",
    "for i in range(number_target_updates):\n",
    "    replay_buffer = []\n",
    "\n",
    "    for k in range(number_gradient_steps):\n",
    "        print(\"iteration number\", k)\n",
    "\n",
    "        batch_adv_n = []\n",
    "        iteration_winners = Counter({0:0,1:0,2:0})\n",
    "\n",
    "        tic = time.time()\n",
    "        for i in range(number_updates_per_expert_update):\n",
    "            #Player and Opponent play games, player follows the policy\n",
    "            #Illegal moves are never played\n",
    "            paths, batch_winners = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps=100)\n",
    "\n",
    "            #The winners are recorded for recordkeeping\n",
    "            iteration_winners += batch_winners\n",
    "            \n",
    "            #Add new rollouts to the replay buffer\n",
    "            replay_buffer += paths\n",
    "\n",
    "            for path in paths:\n",
    "#                print(path['reward'])\n",
    "                path['reward'] = np.array(path['reward']) - sess.run(critic, feed_dict ={observation_placeholder:path['observation']}).flatten()\n",
    "#                print(path['reward'])\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            #The data of the paths are extracted\n",
    "            adv_n = Policy_Gradient.sum_of_rewards(paths)\n",
    "            batch_adv_n = batch_adv_n + adv_n\n",
    "            boards = np.concatenate([path['observation'] for path in paths])\n",
    "            masks = np.concatenate([path['mask'] for path in paths])\n",
    "            actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "\n",
    "            #The paths data are fed into the model for updating\n",
    "            sess.run(update_op_actor, feed_dict = {mask_placeholder: masks, adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "\n",
    "\n",
    "        #Unwind win data:\n",
    "    #     print(iteration_winners)\n",
    "        print(\"mean adv\", np.mean(batch_adv_n))\n",
    "        print(\"iteration time\", time.time() - tic)\n",
    "    #     print(paths[0])\n",
    "\n",
    "\n",
    "    #     expert_player = Players.Expert_Player()\n",
    "    #     _, expert_batch_winners = multiplayer_tools.batch_rollout(player, expert_player, env, max_time_steps=900)\n",
    "    #     player_loss_percentage_vs_expert = expert_batch_winners[2]*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "    #     print(\"loss percent vs expert\", player_loss_percentage_vs_expert)\n",
    "        opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder)\n",
    "    \n",
    "    #Collect samples from our replay buffer\n",
    "    states, actions, next_states, rewards, masks, not_end_of_path = DQN.sample_paths(replay_buffer, batch_size = 100)\n",
    "\n",
    "    #Compute target values\n",
    "    target_values = DQN.compute_target_values(player, next_states, masks, not_end_of_path, rewards, verbose=False)\n",
    "\n",
    "    #Update the network\n",
    "    player.session.run(update_op_critic, feed_dict= {observation_placeholder : states, action_placeholder : actions, target_placeholder : target_values })\n",
    "    print(\"Size of replay buffer:\", len(replay_buffer))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
