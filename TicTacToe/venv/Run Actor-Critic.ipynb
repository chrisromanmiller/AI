{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "\n",
    "\n",
    "import multiplayer_tools\n",
    "import TicTacToe\n",
    "import Players\n",
    "import Policy_Gradient\n",
    "import DQN\n",
    "import Actor_Critic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 0\n",
      "mean adv -0.49869193621992947\n",
      "iteration time 1.0803070068359375\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "iteration number 1\n",
      "mean adv -0.4999143487750607\n",
      "iteration time 1.5056378841400146\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 2\n",
      "mean adv -0.47620878056361765\n",
      "iteration time 1.577125072479248\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 3\n",
      "mean adv -0.5044455023502384\n",
      "iteration time 1.5549168586730957\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 4\n",
      "mean adv -0.48770512196468935\n",
      "iteration time 1.5819599628448486\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "Size of replay buffer: 1428\n",
      "iteration number 0\n",
      "mean adv -0.4869562708568011\n",
      "iteration time 1.4593772888183594\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 1\n",
      "mean adv -0.4952364306069728\n",
      "iteration time 1.8744378089904785\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 2\n",
      "mean adv -0.5183437110932982\n",
      "iteration time 1.5084750652313232\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 3\n",
      "mean adv -0.540119156467891\n",
      "iteration time 2.242213726043701\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "iteration number 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c8b26257e5c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;31m#Player and Opponent play games, player follows the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;31m#Illegal moves are never played\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_winners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiplayer_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m#The winners are recorded for recordkeeping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/multiplayer_tools.py\u001b[0m in \u001b[0;36mbatch_rollout\u001b[0;34m(player, opponent, env, max_time_steps)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mbatch_winners\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_winner\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/multiplayer_tools.py\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(player, opponent, env)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menvironments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Need to record final reward for player 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/TicTacToe.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m#internal commands which test for termination conditions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0m_has_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_has_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0m_full_board\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_row\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "reload(Policy_Gradient)\n",
    "reload(DQN)\n",
    "reload(Players)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import multiplayer_tools\n",
    "reload(Policy_Gradient)\n",
    "reload(DQN)\n",
    "reload(Players)\n",
    "reload(TicTacToe)\n",
    "\n",
    "\n",
    "\n",
    "#Main code for running policy gradient\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#Define the placeholders\n",
    "observation_placeholder = tf.placeholder(shape = [None,2 , 3,3], dtype = tf.int32, name ='observe')\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32, name ='adv_n')\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32, name ='action')\n",
    "mask_placeholder = tf.placeholder(shape=[None, 9], dtype = tf.int32, name= 'mask')\n",
    "target_placeholder = tf.placeholder(shape = [None,], dtype = tf.float32, name = 'target')\n",
    "\n",
    "\n",
    "#Define the model, action distribution\n",
    "model = Policy_Gradient.TicTacToe_model(observation_placeholder, \"actor\", 9)\n",
    "model_input_s = Policy_Gradient.policy_distribution(model)\n",
    "\n",
    "critic = Actor_Critic.TicTacToe_model(observation_placeholder, \"critic\", 1) \n",
    "\n",
    "\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob, entropy = Policy_Gradient.get_log_prob(model, action_placeholder, mask_placeholder)\n",
    "loss, update_op_actor = Policy_Gradient.loss_and_update_op(log_prob, entropy, adv_n_placeholder, entropy_coeff = 0)\n",
    "update_op_critic = DQN.symbolic_Q_update(critic, target_placeholder, action_placeholder)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Defines player, opponent\n",
    "player = Players.NN_Player(model, model_input_s, sess, observation_placeholder, duplicate=False, deterministic = False)\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.mnk_game()\n",
    "\n",
    "#Set parameters for update\n",
    "number_target_updates = 1000\n",
    "number_gradient_steps = 5\n",
    "number_updates_per_expert_update = 10\n",
    "\n",
    "\n",
    "for i in range(number_target_updates):\n",
    "    replay_buffer = []\n",
    "\n",
    "    for k in range(number_gradient_steps):\n",
    "        print(\"iteration number\", k)\n",
    "\n",
    "        batch_adv_n = []\n",
    "        iteration_winners = Counter({0:0,1:0,2:0})\n",
    "\n",
    "        tic = time.time()\n",
    "        for i in range(number_updates_per_expert_update):\n",
    "            #Player and Opponent play games, player follows the policy\n",
    "            #Illegal moves are never played\n",
    "            paths, batch_winners = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps=100)\n",
    "\n",
    "            #The winners are recorded for recordkeeping\n",
    "            iteration_winners += batch_winners\n",
    "            \n",
    "            #Add new rollouts to the replay buffer\n",
    "            replay_buffer += paths\n",
    "\n",
    "            for path in paths:\n",
    "#                print(path['reward'])\n",
    "                path['reward'] = np.array(path['reward']) - sess.run(critic, feed_dict ={observation_placeholder:path['observation']}).flatten()\n",
    "#                print(path['reward'])\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            #The data of the paths are extracted\n",
    "            adv_n = Policy_Gradient.sum_of_rewards(paths)\n",
    "            batch_adv_n = batch_adv_n + adv_n\n",
    "            boards = np.concatenate([path['observation'] for path in paths])\n",
    "            masks = np.concatenate([path['mask'] for path in paths])\n",
    "            actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "\n",
    "            #The paths data are fed into the model for updating\n",
    "            sess.run(update_op_actor, feed_dict = {mask_placeholder: masks, adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "\n",
    "\n",
    "        #Unwind win data:\n",
    "    #     print(iteration_winners)\n",
    "        print(\"mean adv\", np.mean(batch_adv_n))\n",
    "        print(\"iteration time\", time.time() - tic)\n",
    "    #     print(paths[0])\n",
    "\n",
    "\n",
    "    #     expert_player = Players.Expert_Player()\n",
    "    #     _, expert_batch_winners = multiplayer_tools.batch_rollout(player, expert_player, env, max_time_steps=900)\n",
    "    #     player_loss_percentage_vs_expert = expert_batch_winners[2]*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "    #     print(\"loss percent vs expert\", player_loss_percentage_vs_expert)\n",
    "        opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder)\n",
    "    \n",
    "    #Collect samples from our replay buffer\n",
    "    states, actions, next_states, rewards, masks, not_end_of_path = DQN.sample_paths(replay_buffer, batch_size = 100)\n",
    "\n",
    "    #Compute target values\n",
    "    target_values = DQN.compute_target_values(player, next_states, masks, not_end_of_path, rewards, verbose=False)\n",
    "\n",
    "    #Update the network\n",
    "    player.session.run(update_op_critic, feed_dict= {observation_placeholder : states, action_placeholder : actions, target_placeholder : target_values })\n",
    "    print(\"Size of replay buffer:\", len(replay_buffer))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
