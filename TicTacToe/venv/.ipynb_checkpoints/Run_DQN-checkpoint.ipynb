{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "from importlib import reload\n",
    "\n",
    "import multiplayer_tools\n",
    "import tictactoe\n",
    "import Players\n",
    "import Policy_Gradient\n",
    "import DQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "Destroying NN_Player and Session...\n",
      "Destroying NN_Player and Session...\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n",
      "replacing \n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying NN_Player and Session...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-edae299fd512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#Collect rollouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mtic_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiplayer_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m#Add rollouts to the replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/multiplayer_tools.py\u001b[0m in \u001b[0;36mbatch_rollout\u001b[0;34m(player, opponent, env, max_time_steps)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtime_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime_steps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_time_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mbatch_winners\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_winner\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/multiplayer_tools.py\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(player, opponent, env)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mplayer_has_acted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlegal_moves\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0macs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/Players.py\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(self, observations, legal_moves, epsilon)\u001b[0m\n\u001b[1;32m    362\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrandom_float\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                     \u001b[0marg_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                     \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marg_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m     \"\"\"\n\u001b[1;32m    945\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmaximum\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0malong\u001b[0m \u001b[0man\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tictactoe \n",
    "tf.reset_default_graph()\n",
    "reload(Policy_Gradient)\n",
    "reload(DQN)\n",
    "reload(Players)\n",
    "reload(tictactoe)\n",
    "from collections import Counter\n",
    "\n",
    "#Define the placeholders\n",
    "#TODO: update placeholders\n",
    "observation_placeholder = tf.placeholder(shape = [None,2, 3,3], dtype = tf.int32, name = \"obs_placeholder\")\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32, name = \"act_placeholder\")\n",
    "\n",
    "#target place holder is r(s,a) + \\gamma \\max_a Q(s',a)\n",
    "target_placeholder = tf.placeholder(shape = [None], dtype = tf.float32, name = \"target_placeholder\")\n",
    "\n",
    "#Define the model and loss function\n",
    "model = Policy_Gradient.TicTacToe_model(observation_placeholder, scope = \"Q_learn\")\n",
    "update_op = DQN.symbolic_Q_update(model, target_placeholder, action_placeholder)\n",
    "\n",
    "#Start a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Define the players\n",
    "player = Players.NN_Player(model, model, sess, observation_placeholder, duplicate = False)\n",
    "opponent = Players.Random_Player()\n",
    "judge =  Players.NN_Player(model, model, player.session, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "#Define the environment\n",
    "env = tictactoe.mnk_game()\n",
    "\n",
    "#Load current net\n",
    "temp_file_name = './bot_10_31_q_v2.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "#saver = tf.train.Saver()\n",
    "#saver.restore(sess, temp_file_name)\n",
    "\n",
    "replay_buffer = []\n",
    "first_state = None\n",
    "step = 0\n",
    "tic = time.time()\n",
    "total_buffer_time = 0\n",
    "total_runtime = 0\n",
    "while True:\n",
    "    step += 1\n",
    "\n",
    "    #Collect rollouts\n",
    "    tic_buffer = time.time()\n",
    "    paths, _ = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps = 300)\n",
    "\n",
    "    #Add rollouts to the replay buffer\n",
    "    if len(replay_buffer) > 100000:\n",
    "        replay_buffer = replay_buffer[30000:]\n",
    "    \n",
    "    replay_buffer += paths\n",
    "    #Collect samples from our replay buffer\n",
    "    states, actions, next_states, rewards, masks, not_end_of_path = DQN.sample_paths(replay_buffer, batch_size = 100)\n",
    "    total_buffer_time +=  (time.time() - tic_buffer)\n",
    "\n",
    "    \n",
    "    tic_runtime = time.time()\n",
    "    #Compute target values\n",
    "    target_values = DQN.compute_target_values(judge, next_states, masks, not_end_of_path, rewards, verbose=False)\n",
    "\n",
    "    #Update the network\n",
    "    player.session.run(update_op, feed_dict= {observation_placeholder : states, action_placeholder : actions, target_placeholder : target_values })\n",
    "    total_runtime += (time.time() - tic_runtime)\n",
    "    \n",
    "    if step % 10 ==0:\n",
    "        _, player_batch_winners = multiplayer_tools.batch_rollout(player, opponent, env, max_time_steps=1000)\n",
    "        _, judge_batch_winners = multiplayer_tools.batch_rollout(judge, opponent, env, max_time_steps=1000)\n",
    "        frac_player_loss = (player_batch_winners[2]*1.0)/(player_batch_winners[0] + player_batch_winners[1] + player_batch_winners[2])\n",
    "        frac_judge_loss = (judge_batch_winners[2]*1.0)/(judge_batch_winners[0] + judge_batch_winners[1] + judge_batch_winners[2])\n",
    "        if frac_player_loss <= frac_judge_loss:\n",
    "            print(\"replacing \")\n",
    "            judge = Players.NN_Player(model, model, player.session, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "    #Occasionally, test the model and replace it with a previous iteration\n",
    "    if step% 100 ==0:\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        print(\"Time since last update\", time.time() - tic)\n",
    "        print(\"Total gradient step runtime:\",total_runtime)\n",
    "        print(\"Total buffer time:\", total_buffer_time)\n",
    "        total_runtime = 0\n",
    "        total_buffer_time = 0\n",
    "        tic = time.time()\n",
    "        step = 0\n",
    "        \n",
    "        #Print current board distribution\n",
    "        initial_board = np.array(sess.run(model, feed_dict= {observation_placeholder : [np.array([[[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0]]]) == 1] }))\n",
    "        print(np.reshape(initial_board, newshape = (3,3)))\n",
    "\n",
    "#        player.epsilon = 0\n",
    "#        expert_player = Players.Expert_Player()\n",
    "#        child_player = Players.Child_Player()\n",
    "#        _, expert_batch_winners = multiplayer_tools.batch_rollout(player, expert_player, env, max_time_steps=500)\n",
    "#        _, child_batch_winners = multiplayer_tools.batch_rollout(player, child_player, env, max_time_steps=500)\n",
    "#         print(expert_batch_winners)\n",
    "#        expert_batch_percentages = np.array([expert_batch_winners[0], expert_batch_winners[1], expert_batch_winners[2]])*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "#        child_batch_percentages = np.array([child_batch_winners[0], child_batch_winners[1], child_batch_winners[2]])*1.0/(child_batch_winners[0] + child_batch_winners[1] + child_batch_winners[2])\n",
    "#        player.epsilon = .2\n",
    "#        print(\"percentages against expert\", expert_batch_percentages.tolist())\n",
    "#        print(\"percentages against child\", child_batch_percentages.tolist())\n",
    "\n",
    "        print(\"Size of replay buffer:\", len(replay_buffer))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "#         _, player_batch_winners = batch_rollout(player, expert_player, env, max_time_steps=1000)\n",
    "#         frac_player_loss = (player_batch_winners[2]*1.0)/(player_batch_winners[0] + player_batch_winners[1] + player_batch_winners[2])\n",
    "#        print(player_batch_winners)\n",
    "#         _, opponent_batch_winners = batch_rollout(opponent, expert_player, env, max_time_steps=1000)\n",
    "#         frac_opponent_loss = (opponent_batch_winners[2]*1.0)/(opponent_batch_winners[0] + opponent_batch_winners[1] + opponent_batch_winners[2])\n",
    "        \n",
    "        \n",
    "#         if frac_player_loss <= frac_opponent_loss:\n",
    "        opponent = Players.NN_Player(model, model, sess, observation_placeholder, duplicate = True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
