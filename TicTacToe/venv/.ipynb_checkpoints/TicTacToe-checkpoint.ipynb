{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe\n",
    "import Players\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, num_actions, scope, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 9 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "# def update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter):\n",
    "#     old_model = new_model\n",
    "#     old_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter-1))\n",
    "#     \n",
    "#     new_model = TicTacToe_model(board_placeholder, 9, scope = \"model\" + str(update_iter))\n",
    "#     \n",
    "#     new_model_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter))\n",
    "#     sess.run(tf.variables_initializer(new_model_vars))\n",
    "#     \n",
    "#     for var_old, var_new in zip(old_model_vars, new_model_vars):\n",
    "#         var_new = sess.run(var_old)\n",
    "# #        print(sess.run(var_old))\n",
    "# #        print(sess.run(var_new))\n",
    "# #    print(sess.run(old_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "# #    print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#     return old_model, new_model\n",
    "# \n",
    "# def update_and_duplicate_old_model():\n",
    "#     old_model = new_model\n",
    "#     \n",
    "#     variables = tf.trainable_variables()\n",
    "#     for var1 in variables:\n",
    "#         if \"pred/\" in var1.name:\n",
    "#             trained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\n",
    "#             value = sess.run(trained_var)\n",
    "#             sess.run(tf.assign(var1, value))\n",
    "\n",
    "\n",
    "def sample_action(model, mask_placeholder):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(probs=maskedSoftmax(out, mask_placeholder))\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "\"\"\"Code from online\"\"\"\n",
    "def maskedSoftmax(logits, mask):\n",
    "    \"\"\"\n",
    "    Masked softmax over dim 1\n",
    "    :param logits: (N, L)\n",
    "    :param mask: (N, L)\n",
    "    :return: probabilities (N, L)\n",
    "    \"\"\"\n",
    "    indices = tf.where(mask)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    result = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "    result.set_shape(logits.shape)\n",
    "    return result\n",
    "\n",
    "# def sample_trajectory_random_opponent(new_model, sess, env):\n",
    "#     new_sample = sample_action(new_model)\n",
    "#     obs, acs, rewards = [], [], []\n",
    "#     seed = np.random.randint(0,2)\n",
    "#     ob = env.reset()\n",
    "#     if seed % 2 == 0:\n",
    "#         while True:\n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#             \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#     else:\n",
    "#         while True:       \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#    \n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#                 \n",
    "#     path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "#                 \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "#                 \"action\" : np.array(acs, dtype=np.float32)}\n",
    "# #    print(str(len(path[\"observation\"])))\n",
    "#     return path\n",
    "    \n",
    "\n",
    "def sample_trajectory(player, opponent, env):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards = [], [], []\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        if env.current_player == 1:\n",
    "            obs.append(ob)\n",
    "            player_action = player.policy(np.array([ob]), np.array([env.legal_moves()]))\n",
    "            acs.append(player_action[0])\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        else:\n",
    "            opponent_action = opponent.policy(np.array([ob]), np.array([env.legal_moves()]))\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "    return path\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# def sample_trajectories(player, opponent, env, min_timesteps_per_batch):\n",
    "#     paths =[]\n",
    "#     timesteps_this_batch = 0\n",
    "#     while True:\n",
    "#         path = sample_trajectory(player, opponent, env)\n",
    "#         paths.append(path)\n",
    "#         timesteps_this_batch += len(path['observation'])\n",
    "#         if timesteps_this_batch > min_timesteps_per_batch:\n",
    "#             break\n",
    "#     return paths\n",
    "    \n",
    "def sum_of_rewards(paths, gamma = 1): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder, mask_placeholder):\n",
    "    action_dim =9 \n",
    "    logits = model\n",
    "    \n",
    "    indices = tf.where(mask_placeholder)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    result = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "    result.set_shape(logits.shape)\n",
    "    \n",
    "    \"\"\"Want to emulate this:\"\"\"\n",
    "    probability_dist = tf.nn.softmax(logits)\n",
    "    legal_pseudo_probability_dist = probability_dist*values\n",
    "    legalprobability_dist = tf.divide(legal_pseudo_probability_dist, tf.reduce_sum(legal_pseudo_probability_dist, axis= 1))\n",
    "    \n",
    "    log_prob = -tf.log(tf.reduce_sum(legalprobability_dist * tf.one_hot(action_placeholder, action_dim ) , axis = 1 ) )\n",
    "#    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= tf.SparseTensor(indices, values, denseShape))\n",
    "    return log_prob\n",
    "\n",
    "def loss_and_update_op(log_prob,adv_n):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) \n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "iteration number 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (9,) for Tensor 'Placeholder_3:0', which has shape '(?, 9)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-dc6a5cbc69ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mtime_step\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_time_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mtime_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-4ff219449e3c>\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(player, opponent, env)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mopponent_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_moves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m             \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopponent_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/ai/TicTacToe/venv/Players.py\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(self, observations, legal_moves)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# dist = tf.distributions.Categorical(logits=self.model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_sample_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlegal_moves\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1074\u001b[0m                              \u001b[0;34m'which has shape %r'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[0;32m-> 1076\u001b[0;31m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m   1077\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (9,) for Tensor 'Placeholder_3:0', which has shape '(?, 9)'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models *symbolically*\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "mask_placeholder = tf.placeholder(shape=[None, 9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "model_input_s = sample_action(model, mask_placeholder)\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(model, action_placeholder, mask_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#Defines player, opponent\n",
    "player = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder, duplicate=False)\n",
    "opponent = Players.Random_Player()\n",
    "opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder)\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 200\n",
    "number_expert_updates = 100\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "\n",
    "    #Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "#     if k ==0:\n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "#     else:\n",
    "#         sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "    \n",
    "    batch_adv_n = []\n",
    "    batch_illegal = []\n",
    "    batch_wins = []\n",
    "    \n",
    "    tic = time.time()\n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        #Produce some trajectories\n",
    "#        print(\"sampling games...\")\n",
    "        max_time_step = 100\n",
    "        time_step = 0\n",
    "        paths = []\n",
    "        while time_step < max_time_step:\n",
    "            path = sample_trajectory(player,opponent,env)\n",
    "            paths.append(path)\n",
    "            time_step += len(path['reward'])\n",
    "            batch_wins.append(env.current_winner)\n",
    "            batch_illegal.append(env.illegal_moves_N[0])\n",
    "#         paths = sample_trajectories(player, opponent, env, 100)\n",
    "\n",
    "        adv_n = sum_of_rewards(paths)\n",
    "        batch_adv_n = batch_adv_n + adv_n\n",
    "        \n",
    "\n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "\n",
    "\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "\n",
    "    \n",
    "    #Unwind win data:\n",
    "    batch_win_unique, batch_win_counts = np.unique(batch_wins, return_counts=True)\n",
    "    batch_win_percentage = dict(zip(batch_win_unique, batch_win_counts))[1]*1.0/len(batch_wins)\n",
    "    if batch_win_percentage > .6:\n",
    "        opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder)\n",
    "            \n",
    "    print(\"mean adv\", np.mean(batch_adv_n))\n",
    "    print(\"mean illegal\", np.mean(batch_illegal))\n",
    "    print(\"win percentage\", batch_win_percentage)\n",
    "    print(\"iteration time\", time.time() - tic)\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./bot_10_28_v2.ckpt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_file_name = './bot_10_28_v2.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(a.name, sess.run(a)) for i, a  in enumerate(tf.trainable_variables())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[16].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_log_prob() missing 1 required positional argument: 'mask_placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9ea0df022140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#Define Loss functions *symbolically*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_and_update_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_n_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_log_prob() missing 1 required positional argument: 'mask_placeholder'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(new_model, action_placeholder, mask_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "        #Produce some trajectories\n",
    "        paths = sample_trajectories_random_opponent(new_model, sess, 20, env = env)\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"model\" +str(-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_trajectory(player, opponent, env):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards = [], [], []\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        if env.current_player == 1:\n",
    "            obs.append(ob)\n",
    "            player_action = player.policy(np.array([ob]))\n",
    "            acs.append(player_action[0])\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        else:\n",
    "            opponent_action = opponent.policy(np.array([ob]), env.legal_moves())\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
