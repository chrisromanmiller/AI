{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermiller/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe\n",
    "from collections import Counter\n",
    "import Players\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, scope, reuse=tf.AUTO_REUSE, num_actions = 9):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, num_actions , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "def sample_action(model, mask_placeholder):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(probs=maskedSoftmax(out, mask_placeholder))\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "\"\"\"Code from online\"\"\"\n",
    "def maskedSoftmax(logits, mask):\n",
    "    \"\"\"\n",
    "    Masked softmax over dim 1\n",
    "    :param logits: (N, L)\n",
    "    :param mask: (N, L)\n",
    "    :return: probabilities (N, L)\n",
    "    \"\"\"\n",
    "    indices = tf.where(mask)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    result = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "    result.set_shape(logits.shape)\n",
    "    return result\n",
    "\n",
    "\n",
    "def batch_rollout(player,opponent, env, max_time_steps = 100, exploration_on =False, epsilon =.1):\n",
    "    paths = []\n",
    "    batch_winners = Counter({0: 0, 1: 0, 2:0})\n",
    "    time_steps = 0\n",
    "    while time_steps < max_time_steps:\n",
    "        path = sample_trajectory(player,opponent,env, exploration_on, epsilon)\n",
    "        paths += [path]\n",
    "        batch_winners[env.current_winner] +=1\n",
    "        time_steps += len(path['observation'])\n",
    "    return paths, batch_winners\n",
    "    \n",
    "    \n",
    "    \n",
    "def sample_trajectory(player, opponent, env, exploration_on = False, epsilon = .1):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards, masks = [], [], [], []\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    player_has_acted = False\n",
    "    action = None\n",
    "    \n",
    "    #Do rest of moves\n",
    "    while not done:\n",
    "        #Get current observation of current player\n",
    "        ob = env.get_observation(env.current_player)\n",
    "        legal_moves = env.legal_moves()\n",
    "        if env.current_player == 1:\n",
    "            #Reward is recorded as results of state,action pair... need to check player 1 has acted already\n",
    "            if player_has_acted:\n",
    "                rewards.append(env.get_reward(1))\n",
    "            else:\n",
    "                player_has_acted = True\n",
    "                \n",
    "            action = player.policy(np.array([ob]), np.array([legal_moves]))\n",
    "            if exploration_on:\n",
    "                legal_options = np.array(legal_moves)\n",
    "                action = [random.choice(np.nonzero(legal_options)[0])]\n",
    "            obs.append(ob)\n",
    "            acs.append(action[0])\n",
    "            masks.append(legal_moves)\n",
    "        else:\n",
    "            action = opponent.policy(np.array([ob]), np.array([legal_moves]))\n",
    "        done, _ = env.step(action[0]) \n",
    "\n",
    "    #Need to record final reward for player 1\n",
    "    rewards.append(env.get_reward(1))\n",
    "    \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.int32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.int32),\n",
    "                \"mask\" : np.array(masks, dtype=np.int32)}\n",
    "    return path\n",
    "\n",
    "    \n",
    "    \n",
    "def sum_of_rewards(paths, gamma = .6): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder, mask_placeholder):\n",
    "    action_dim = 9 \n",
    "    logits = model\n",
    "    \n",
    "    indices = tf.where(mask_placeholder)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    probability_dist = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "#     probability_dist = probability_dist.set_shape(logits.shape)\n",
    "    log_probability_dist = tf.scatter_nd(sparseResult.indices, tf.log(sparseResult.values), sparseResult.dense_shape)\n",
    "\n",
    "    \"\"\"Want to emulate this:\"\"\"\n",
    "#     probability_dist = tf.nn.softmax(logits)\n",
    "#     legal_pseudo_probability_dist = probability_dist*values\n",
    "#     legalprobability_dist = tf.divide(legal_pseudo_probability_dist, tf.reduce_sum(legal_pseudo_probability_dist, axis= 1))\n",
    "    \n",
    "    prod = tf.multiply(probability_dist, tf.one_hot(action_placeholder, action_dim ))\n",
    "    \n",
    "    entropy = - tf.reduce_sum(probability_dist * log_probability_dist, axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_prob = tf.log(tf.reduce_sum(prod , axis = 1 ))\n",
    "#    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= tf.SparseTensor(indices, values, denseShape))\n",
    "    return log_prob, entropy\n",
    "\n",
    "def loss_and_update_op(log_prob, entropy, adv_n, entropy_coeff = .1):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) -  entropy_coeff * entropy\n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "iteration number 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c5d94b2a777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0miteration_winners\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_winners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_of_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mbatch_adv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_adv_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madv_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ff256ebc2ce1>\u001b[0m in \u001b[0;36msum_of_rewards\u001b[0;34m(paths, gamma)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mweighted_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mq_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madv_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1868\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1869\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Main code for running policy gradient\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models *symbolically*\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "mask_placeholder = tf.placeholder(shape=[None, 9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"policy_gradient\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "model_input_s = sample_action(model, mask_placeholder)\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob, entropy = get_log_prob(model, action_placeholder, mask_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, entropy, adv_n_placeholder, entropy_coeff = 0)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#Defines player, opponent\n",
    "player = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder, duplicate=False)\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "#Loads old player,opponent\n",
    "# temp_file_name = './bot_10_28_v6.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, temp_file_name)\n",
    "\n",
    "\n",
    "# opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 5\n",
    "number_expert_updates = 1000\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "    \n",
    "    batch_adv_n = []\n",
    "    iteration_winners = Counter({0:0,1:0,2:0})\n",
    "    \n",
    "    tic = time.time()\n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        paths, batch_winners = batch_rollout(player, opponent, env, max_time_steps=1000)\n",
    "        iteration_winners += batch_winners\n",
    "        \n",
    "        adv_n = sum_of_rewards(paths)\n",
    "        batch_adv_n = batch_adv_n + adv_n\n",
    "        \n",
    "\n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        masks = np.concatenate([path['mask'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        \n",
    "        sess.run(update_op, feed_dict = {mask_placeholder: masks, adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "    \n",
    "    \n",
    "    #Unwind win data:\n",
    "#     print(iteration_winners)\n",
    "    print(\"mean adv\", np.mean(batch_adv_n))\n",
    "    print(\"iteration time\", time.time() - tic)\n",
    "#     print(paths[0])\n",
    "    \n",
    "    \n",
    "    expert_player = Players.Expert_Player()\n",
    "    _, expert_batch_winners = batch_rollout(player, expert_player, env, max_time_steps=900, exploration_on =True, )\n",
    "    player_loss_percentage_vs_expert = expert_batch_winners[2]*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "    print(\"loss percent vs expert\", player_loss_percentage_vs_expert)\n",
    "    opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder,mask_placeholder)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./bot_10_28_v6.ckpt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save current net\n",
    "\n",
    "temp_file_name = './bot_10_28_v7.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bot_10_28_v3.ckpt\n"
     ]
    }
   ],
   "source": [
    "#Load current net\n",
    "\n",
    "temp_file_name = './bot_10_28_v3.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 867, 1: 0, 2: 9133}\n"
     ]
    }
   ],
   "source": [
    "#Cell Tests Players against each other\n",
    "\n",
    "from importlib import reload\n",
    "reload(Players)\n",
    "player = Players.Random_Player()\n",
    "opponent = Players.Expert_Player()\n",
    "\n",
    "env = TicTacToe.TicTacToe()\n",
    "batch_winners = {0:0,1:0,2:0}\n",
    "for i in range(10000):\n",
    "    path = sample_trajectory(player,opponent,env)\n",
    "    batch_winners[env.current_winner] += 1\n",
    "print(batch_winners)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Q-learning....\n",
    "from importlib import reload \n",
    "reload(Players)\n",
    "\n",
    "def collect_tuples(model, observation_placeholder, opponent, env, sess, num_tuples =100, exploration =.1):\n",
    "    current_state1 = None\n",
    "    current_action = None\n",
    "    current_reward = None\n",
    "    current_state2 = None\n",
    "    tuples = [] #or maybe an array?\n",
    "    ob = env.reset\n",
    "    i=0\n",
    "    while i< num_tuples:\n",
    "        i+=1\n",
    "        #Record the current state\n",
    "        current_state1 = ob\n",
    "\n",
    "        #We pick the best action epsilon-greedily\n",
    "        action = sess.run( model, feed_dict = {observation_placeholder: [ob]})[0] #or something\n",
    "        if np.random.uniform() < exploration:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        #Record the current action and reward\n",
    "        ob = env.step(action)\n",
    "        current_action = action\n",
    "        current_reward = env.get_reward() #or whatever\n",
    "        \n",
    "        #Record the new state\n",
    "        current_state2 = ob\n",
    "        tuples.append({'state1': current_state1, 'action':current_action, 'state2':current_state2, 'reward':current_reward})\n",
    "        \n",
    "    return tuples\n",
    "    \n",
    "def arg_max_sample(model, mask_placeholder):\n",
    "    indices = tf.where(mask_placeholder)\n",
    "    values = tf.gather_nd(model, indices)\n",
    "    denseShape = tf.cast(tf.shape(model), tf.int64)\n",
    "    x = tf.SparseTensor(indices, values, denseShape)\n",
    "    x = tf.scatter_nd(x.indices, x.values, x.dense_shape)\n",
    "    return tf.argmax(x, 1)\n",
    "\n",
    "def get_loss_and_optimizer_Q(model, targets_placeholder, action_placeholder):\n",
    "    diff = tf.subtract(model[action_placeholder], targets_placeholder)\n",
    "    loss = tf.norm(diff)\n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "\n",
    "def compute_target_values(arg_max_sample_s , next_state, mask, not_end_of_path, reward, decay = .01):\n",
    "    future_expected_reward = sess.run(arg_max_sample_s, feed_dict= {observation_placeholder: next_state, mask_placeholder: mask})\n",
    "    return reward + decay * not_end_of_path * future_expected_reward\n",
    "\n",
    "def sample_paths(paths, batch_size = 10):\n",
    "    #Make the easy lists\n",
    "    observation_list = np.concatenate([path['observation'] for path in paths])\n",
    "    action_list = np.concatenate([path['action'] for path in paths])\n",
    "    reward_list = np.concatenate([path['reward'] for path in paths])\n",
    "    mask_list = np.concatenate([path['mask'] for path in paths])\n",
    "\n",
    "    #Make the done list\n",
    "    #Returns 0 if at the terminal step\n",
    "    #Returns 1 otherwise.\n",
    "    list_of_ones = [1] * len(observation_list)\n",
    "    partial_sum =0\n",
    "    for path in paths:\n",
    "        partial_sum += len(path['observation'])\n",
    "        list_of_ones[partial_sum-1] = 0\n",
    "    done_list = list_of_ones\n",
    "    \n",
    "    #Get some random indices\n",
    "    indices = np.random.choice(len(observation_list), batch_size) \n",
    "    \n",
    "    #Select randomly chosen entries\n",
    "    state1 = np.array([observation_list[i] for i in indices])\n",
    "    action = np.array([action_list[i] for i in indices])\n",
    "    state2 = np.array([observation_list[(i+1) % len(observation_list)] for i in indices])\n",
    "    reward = np.array([reward_list[i] for i in indices])\n",
    "    mask = np.array([mask_list[(i+1) % len(observation_list)] for i in indices])\n",
    "    done = np.array([done_list[i] for i in indices])\n",
    "    return state1, action, state2 , reward, mask, done\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-d0902b408d67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#NEED: call batch_rollouts(), feed into chris function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_rollout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_end_of_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#Define any constants\n",
    "gamma = .95\n",
    "\n",
    "#Define the placeholders\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "\n",
    "#target place holder is r(s,a) + \\gamma \\max_a Q(s',a)\n",
    "target_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "mask_placeholder = tf.placeholder(shape = [None,9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "\n",
    "#Define the model and loss function\n",
    "\n",
    "model = TicTacToe_model(observation_placeholder, scope = \"Q_learn\", reuse=tf.AUTO_REUSE)\n",
    "arg_max_sample_s = arg_max_sample(model, mask_placeholder)\n",
    "# _, update_op, _  = get_loss_and_optimizer_Q(model, target_placeholder, action_placeholder)\n",
    "\n",
    "#Start a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Define the players\n",
    "player = Players.NN_Player(model, arg_max_sample_s, sess, observation_placeholder, mask_placeholder)\n",
    "opponent = Players.Random_Player()\n",
    "env = TicTacToe.TicTacToe()\n",
    "#Collect some tuples\n",
    "# database = []\n",
    "# database += collect_tuples(model, observation_placeholder, opponent, env, sess)\n",
    "# \n",
    "# ###Training time\n",
    "# #I have a tuple, state-action-state from somewhere\n",
    "# sampled_tuple = random.choice(database)\n",
    "# \n",
    "# not_updated_Q = sess.run(player.q_func(), feed_dict = {observation_placeholder: [sampled_tuple['state1']]})\n",
    "# \n",
    "# target_Q = not_updated_Q\n",
    "# target_Q[0, sampled_tuple['action']] =  sampled_tuple['reward'] \n",
    "# target_Q[0, sampled_tuple['action']] += tf.reduce_max(sess.run(player.q_function(), feed_dict = {observation_placeholder: [sampled_tuple['state2']]}), 1)[0]\n",
    "\n",
    "\n",
    "#NEED: call batch_rollouts(), feed into chris function\n",
    "paths = batch_rollout(player, opponent, env)\n",
    "states, actions, next_states, rewards, not_end_of_path, masks = sample_paths(paths, batch_size = 10)\n",
    "\n",
    "\n",
    "\n",
    "target_values = compute_target_values(arg_max_sample_s, next_state, masks, not_end_of_path, rewards)\n",
    "\n",
    "print(target_values)\n",
    "#sess.run(update_op, feed_dict= {observation_placeholder : states, action_placeholder : actions, observation_placeholder : states })\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
