{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermiller/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe\n",
    "import Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, num_actions, scope, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 9 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "# def update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter):\n",
    "#     old_model = new_model\n",
    "#     old_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter-1))\n",
    "#     \n",
    "#     new_model = TicTacToe_model(board_placeholder, 9, scope = \"model\" + str(update_iter))\n",
    "#     \n",
    "#     new_model_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter))\n",
    "#     sess.run(tf.variables_initializer(new_model_vars))\n",
    "#     \n",
    "#     for var_old, var_new in zip(old_model_vars, new_model_vars):\n",
    "#         var_new = sess.run(var_old)\n",
    "# #        print(sess.run(var_old))\n",
    "# #        print(sess.run(var_new))\n",
    "# #    print(sess.run(old_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "# #    print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#     return old_model, new_model\n",
    "# \n",
    "# def update_and_duplicate_old_model():\n",
    "#     old_model = new_model\n",
    "#     \n",
    "#     variables = tf.trainable_variables()\n",
    "#     for var1 in variables:\n",
    "#         if \"pred/\" in var1.name:\n",
    "#             trained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\n",
    "#             value = sess.run(trained_var)\n",
    "#             sess.run(tf.assign(var1, value))\n",
    "\n",
    "\n",
    "def sample_action(model):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(logits=out)\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "# def sample_trajectory_random_opponent(new_model, sess, env):\n",
    "#     new_sample = sample_action(new_model)\n",
    "#     obs, acs, rewards = [], [], []\n",
    "#     seed = np.random.randint(0,2)\n",
    "#     ob = env.reset()\n",
    "#     if seed % 2 == 0:\n",
    "#         while True:\n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#             \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#     else:\n",
    "#         while True:       \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#    \n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#                 \n",
    "#     path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "#                 \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "#                 \"action\" : np.array(acs, dtype=np.float32)}\n",
    "# #    print(str(len(path[\"observation\"])))\n",
    "#     return path\n",
    "    \n",
    "    \n",
    "def sample_trajectory(opponent, new_model, sess, env):\n",
    "    new_sample = sample_action(new_model)\n",
    "    # old_sample = sample_action(old_model)\n",
    "    obs, acs, rewards = [], [], []\n",
    "    seed = np.random.randint(0,2)\n",
    "    ob = env.reset()\n",
    "    if seed % 2 == 0:\n",
    "        while True:\n",
    "            obs.append(ob)\n",
    "            player_action = sess.run(new_sample, feed_dict={observation_placeholder: np.array([ob])})\n",
    "            acs.append(player_action)\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "            tic = time.time()\n",
    "            opponent_action = opponent.policy(np.array([ob]))\n",
    "            print(time.time()-tic)\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "    else:\n",
    "        while True:       \n",
    "            # opponent_action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "            tic = time.time()\n",
    "            opponent_action = opponent.policy(np.array([ob]))\n",
    "            print(time.time()-tic)\n",
    "\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "   \n",
    "            obs.append(ob)\n",
    "            player_action = sess.run(new_sample, feed_dict={observation_placeholder: np.array([ob])})\n",
    "            acs.append(player_action)\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "#    print(str(len(path[\"observation\"])))\n",
    "    return path\n",
    "\n",
    "# def sample_trajectories_random_opponent(new_model, sess, min_timesteps_per_batch, env):\n",
    "#     paths =[]\n",
    "#     timesteps_this_batch = 0\n",
    "#     while True:\n",
    "#         path = sample_trajectory_random_opponent(new_model, sess, env)\n",
    "#         paths.append(path)\n",
    "#         timesteps_this_batch += len(path['observation'])\n",
    "#         if timesteps_this_batch > min_timesteps_per_batch:\n",
    "#             break\n",
    "#     return paths\n",
    "\n",
    "def sample_trajectories(opponent, new_model, sess, min_timesteps_per_batch, env):\n",
    "    paths =[]\n",
    "    timesteps_this_batch = 0\n",
    "    while True:\n",
    "        path = sample_trajectory(opponent, new_model, sess, env)\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += len(path['observation'])\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    return paths\n",
    "    \n",
    "def sum_of_rewards(paths, gamma = .95): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder):\n",
    "    logits = model\n",
    "    \n",
    "    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= logits)\n",
    "    return log_prob\n",
    "\n",
    "def loss_and_update_op(log_prob,adv_n):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) \n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number 0\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "iteration number 1\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n",
      "games sampled.\n",
      "model updated.\n",
      "sampling games...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a58682e9f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#Produce some trajectories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sampling games...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"games sampled.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_of_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9e8ea824b0f5>\u001b[0m in \u001b[0;36msample_trajectories\u001b[0;34m(opponent, new_model, sess, min_timesteps_per_batch, env)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_trajectory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mtimesteps_this_batch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'observation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9e8ea824b0f5>\u001b[0m in \u001b[0;36msample_trajectory\u001b[0;34m(opponent, new_model, sess, env)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mplayer_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mobservation_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0macs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1263\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "    #Define Loss functions *symbolically*\n",
    "    log_prob = get_log_prob(model, action_placeholder)\n",
    "    loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "    #Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "    if k ==0:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "    \n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        #Produce some trajectories\n",
    "        print(\"sampling games...\")\n",
    "        paths = sample_trajectories(opponent, model, sess, 20, env = env)\n",
    "        print(\"games sampled.\")\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "#        print(\"Last game played\", paths)\n",
    "        \n",
    "        #Update the model\n",
    "#        print(\"updating model...\")\n",
    "#        print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#         test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) #, scope=\"model\" +str(k-1))\n",
    "#         for var in test_vars:\n",
    "#             print(var.name, np.max(sess.run(var)))\n",
    "            \n",
    "#        for i, var in enumerate(test_vars):\n",
    "#            print(var.name, sess.run(var))\n",
    "\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "        print(\"model updated.\")\n",
    "    #if new_model does better...\n",
    "    #old_model, model = update_old_model_initialize_new_model(old_model, model, board_placeholder, sess, update_iter= k)\n",
    "    opponent = Players.NN_Player(model, sess, observation_placeholder)\n",
    "    \"\"\" TODO: UPDATE OPPONENT\"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "# for path in paths:\n",
    "#     print(path['observation'])\n",
    "#     print(path['action'])\n",
    "#     print(path['reward'])\n",
    "#     print(\"____\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths[0]['observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(a.name, sess.run(a)) for i, a  in enumerate(tf.trainable_variables())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[16].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0\n",
      "model-1/dense/kernel:0 0.0\n",
      "model-1/dense_1/kernel:0 0.0049994704\n",
      "model-1/dense/kernel:0 1.5649351e-07\n",
      "model-1/dense_1/kernel:0 0.009812419\n",
      "model-1/dense/kernel:0 1.3215144e-07\n",
      "model-1/dense_1/kernel:0 0.014569419\n",
      "model-1/dense/kernel:0 1.2292737e-07\n",
      "model-1/dense_1/kernel:0 0.018785303\n",
      "model-1/dense/kernel:0 1.5019947e-07\n",
      "model-1/dense_1/kernel:0 0.022343298\n",
      "model-1/dense/kernel:0 3.4073742e-07\n",
      "model-1/dense_1/kernel:0 0.025119584\n",
      "model-1/dense/kernel:0 5.161538e-07\n",
      "model-1/dense_1/kernel:0 0.027365364\n",
      "model-1/dense/kernel:0 6.7784526e-07\n",
      "model-1/dense_1/kernel:0 0.029040836\n",
      "model-1/dense/kernel:0 6.912954e-07\n",
      "model-1/dense_1/kernel:0 0.02831384\n",
      "model-1/dense/kernel:0 7.729489e-07\n",
      "model-1/dense_1/kernel:0 0.027713807\n",
      "model-1/dense/kernel:0 1.090002e-06\n",
      "model-1/dense_1/kernel:0 0.027298247\n",
      "model-1/dense/kernel:0 1.6268523e-06\n",
      "model-1/dense_1/kernel:0 0.027223863\n",
      "model-1/dense/kernel:0 1.8749504e-06\n",
      "model-1/dense_1/kernel:0 0.026900835\n",
      "model-1/dense/kernel:0 1.9321071e-06\n",
      "model-1/dense_1/kernel:0 0.02668874\n",
      "model-1/dense/kernel:0 2.022239e-06\n",
      "model-1/dense_1/kernel:0 0.026247295\n",
      "model-1/dense/kernel:0 2.216402e-06\n",
      "model-1/dense_1/kernel:0 0.028726596\n",
      "model-1/dense/kernel:0 2.6245382e-06\n",
      "model-1/dense_1/kernel:0 0.032282468\n",
      "model-1/dense/kernel:0 3.4380334e-06\n",
      "model-1/dense_1/kernel:0 0.035922382\n",
      "model-1/dense/kernel:0 4.1265153e-06\n",
      "model-1/dense_1/kernel:0 0.03935676\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(new_model, action_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "        #Produce some trajectories\n",
    "        paths = sample_trajectories_random_opponent(new_model, sess, 20, env = env)\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"model\" +str(-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
