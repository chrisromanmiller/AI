{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christophermiller/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe\n",
    "from collections import Counter\n",
    "import Players\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, num_actions, scope, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 9 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "# def update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter):\n",
    "#     old_model = new_model\n",
    "#     old_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter-1))\n",
    "#     \n",
    "#     new_model = TicTacToe_model(board_placeholder, 9, scope = \"model\" + str(update_iter))\n",
    "#     \n",
    "#     new_model_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter))\n",
    "#     sess.run(tf.variables_initializer(new_model_vars))\n",
    "#     \n",
    "#     for var_old, var_new in zip(old_model_vars, new_model_vars):\n",
    "#         var_new = sess.run(var_old)\n",
    "# #        print(sess.run(var_old))\n",
    "# #        print(sess.run(var_new))\n",
    "# #    print(sess.run(old_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "# #    print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#     return old_model, new_model\n",
    "# \n",
    "# def update_and_duplicate_old_model():\n",
    "#     old_model = new_model\n",
    "#     \n",
    "#     variables = tf.trainable_variables()\n",
    "#     for var1 in variables:\n",
    "#         if \"pred/\" in var1.name:\n",
    "#             trained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\n",
    "#             value = sess.run(trained_var)\n",
    "#             sess.run(tf.assign(var1, value))\n",
    "\n",
    "\n",
    "def sample_action(model, mask_placeholder):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(probs=maskedSoftmax(out, mask_placeholder))\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "\"\"\"Code from online\"\"\"\n",
    "def maskedSoftmax(logits, mask):\n",
    "    \"\"\"\n",
    "    Masked softmax over dim 1\n",
    "    :param logits: (N, L)\n",
    "    :param mask: (N, L)\n",
    "    :return: probabilities (N, L)\n",
    "    \"\"\"\n",
    "    indices = tf.where(mask)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    result = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "    result.set_shape(logits.shape)\n",
    "    return result\n",
    "\n",
    "# def sample_trajectory_random_opponent(new_model, sess, env):\n",
    "#     new_sample = sample_action(new_model)\n",
    "#     obs, acs, rewards = [], [], []\n",
    "#     seed = np.random.randint(0,2)\n",
    "#     ob = env.reset()\n",
    "#     if seed % 2 == 0:\n",
    "#         while True:\n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#             \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#     else:\n",
    "#         while True:       \n",
    "#             action = [env.action_space.sample()]\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#    \n",
    "#             obs.append(ob)\n",
    "#             action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "#             acs.append(action)\n",
    "#             ob, rew, done, _ = env.step(action[0])\n",
    "#             rewards.append(rew)\n",
    "#             \n",
    "#             if done:\n",
    "#                 break\n",
    "#                 \n",
    "#     path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "#                 \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "#                 \"action\" : np.array(acs, dtype=np.float32)}\n",
    "# #    print(str(len(path[\"observation\"])))\n",
    "#     return path\n",
    "    \n",
    "def batch_rollout(player,opponent, env, max_time_steps = 100, exploration_on =False, epsilon =.1):\n",
    "    paths = []\n",
    "    batch_winners = Counter({0: 0, 1: 0, 2:0})\n",
    "    time_steps = 0\n",
    "    while time_steps < max_time_steps:\n",
    "        path = sample_trajectory(player,opponent,env, exploration_on, epsilon)\n",
    "        paths += [path]\n",
    "        batch_winners[env.current_winner] +=1\n",
    "        time_steps += len(path['observation'])\n",
    "    return paths, batch_winners\n",
    "    \n",
    "    \n",
    "    \n",
    "def sample_trajectory(player, opponent, env, exploration_on = False, epsilon = .1):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards, masks = [], [], [], []\n",
    "    ob = env.reset()\n",
    "    done = False\n",
    "    player_has_acted = False\n",
    "    action = None\n",
    "    \n",
    "    #Do rest of moves\n",
    "    while not done:\n",
    "        #Get current observation of current player\n",
    "        ob = env.get_observation(env.current_player)\n",
    "        legal_moves = env.legal_moves()\n",
    "        if env.current_player == 1:\n",
    "            #Reward is recorded as results of state,action pair... need to check player 1 has acted already\n",
    "            if player_has_acted:\n",
    "                rewards.append(env.get_reward(1))\n",
    "            else:\n",
    "                player_has_acted = True\n",
    "                \n",
    "            action = player.policy(np.array([ob]), np.array([legal_moves]))\n",
    "            if exploration_on:\n",
    "                legal_options = np.array(legal_moves)\n",
    "                action = [random.choice(np.nonzero(legal_options)[0])]\n",
    "            obs.append(ob)\n",
    "            acs.append(action[0])\n",
    "            masks.append(legal_moves)\n",
    "        else:\n",
    "            action = opponent.policy(np.array([ob]), np.array([legal_moves]))\n",
    "        done, _ = env.step(action[0]) \n",
    "\n",
    "    #Need to record final reward for player 1\n",
    "    rewards.append(env.get_reward(1))\n",
    "    \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.int32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.int32),\n",
    "                \"mask\" : np.array(masks, dtype=np.int32)}\n",
    "    return path\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# def sample_trajectories(player, opponent, env, min_timesteps_per_batch):\n",
    "#     paths =[]\n",
    "#     timesteps_this_batch = 0\n",
    "#     while True:\n",
    "#         path = sample_trajectory(player, opponent, env)\n",
    "#         paths.append(path)\n",
    "#         timesteps_this_batch += len(path['observation'])\n",
    "#         if timesteps_this_batch > min_timesteps_per_batch:\n",
    "#             break\n",
    "#     return paths\n",
    "    \n",
    "def sum_of_rewards(paths, gamma = .6): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder, mask_placeholder):\n",
    "    action_dim = 9 \n",
    "    logits = model\n",
    "    \n",
    "    indices = tf.where(mask_placeholder)\n",
    "    values = tf.gather_nd(logits, indices)\n",
    "    denseShape = tf.cast(tf.shape(logits), tf.int64)\n",
    "    \n",
    "    \"\"\"THIS IS THE KEY: tensorflow will automatically set output probabilities to zero of undesignated entries in sparse vector\"\"\"\n",
    "    sparseResult = tf.sparse_softmax(tf.SparseTensor(indices, values, denseShape))\n",
    "    \n",
    "    probability_dist = tf.scatter_nd(sparseResult.indices, sparseResult.values, sparseResult.dense_shape)\n",
    "#     probability_dist = probability_dist.set_shape(logits.shape)\n",
    "    log_probability_dist = tf.scatter_nd(sparseResult.indices, tf.log(sparseResult.values), sparseResult.dense_shape)\n",
    "\n",
    "    \"\"\"Want to emulate this:\"\"\"\n",
    "#     probability_dist = tf.nn.softmax(logits)\n",
    "#     legal_pseudo_probability_dist = probability_dist*values\n",
    "#     legalprobability_dist = tf.divide(legal_pseudo_probability_dist, tf.reduce_sum(legal_pseudo_probability_dist, axis= 1))\n",
    "    \n",
    "    prod = tf.multiply(probability_dist, tf.one_hot(action_placeholder, action_dim ))\n",
    "    \n",
    "    entropy = - tf.reduce_sum(probability_dist * log_probability_dist, axis = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    log_prob = tf.log(tf.reduce_sum(prod , axis = 1 ))\n",
    "#    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= tf.SparseTensor(indices, values, denseShape))\n",
    "    return log_prob, entropy\n",
    "\n",
    "def loss_and_update_op(log_prob, entropy, adv_n, entropy_coeff = .1):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) -  entropy_coeff * entropy\n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: not duplicating session, evaluation will change with tf updates\n",
      "iteration number 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c5d94b2a777>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0miteration_winners\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_winners\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_of_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mbatch_adv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_adv_n\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madv_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-ff256ebc2ce1>\u001b[0m in \u001b[0;36msum_of_rewards\u001b[0;34m(paths, gamma)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0mweighted_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_of_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mq_n\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0madv_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0madv_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m   1866\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1867\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1868\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_gentype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1869\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sum_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models *symbolically*\n",
    "observation_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "mask_placeholder = tf.placeholder(shape=[None, 9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "model = TicTacToe_model(observation_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "#old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "model_input_s = sample_action(model, mask_placeholder)\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob, entropy = get_log_prob(model, action_placeholder, mask_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, entropy, adv_n_placeholder, entropy_coeff = 0)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#Defines player, opponent\n",
    "player = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder, duplicate=False)\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "#Loads old player,opponent\n",
    "# temp_file_name = './bot_10_28_v6.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "# saver = tf.train.Saver()\n",
    "# saver.restore(sess, temp_file_name)\n",
    "\n",
    "\n",
    "# opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder, mask_placeholder)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 5\n",
    "number_expert_updates = 1000\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "    \n",
    "    batch_adv_n = []\n",
    "    iteration_winners = Counter({0:0,1:0,2:0})\n",
    "    \n",
    "    tic = time.time()\n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        paths, batch_winners = batch_rollout(player, opponent, env, max_time_steps=1000)\n",
    "        iteration_winners += batch_winners\n",
    "        \n",
    "        adv_n = sum_of_rewards(paths)\n",
    "        batch_adv_n = batch_adv_n + adv_n\n",
    "        \n",
    "\n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        masks = np.concatenate([path['mask'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        \n",
    "        sess.run(update_op, feed_dict = {mask_placeholder: masks, adv_n_placeholder: adv_n, observation_placeholder: boards , action_placeholder: actions})\n",
    "    \n",
    "    \n",
    "    #Unwind win data:\n",
    "#     print(iteration_winners)\n",
    "    print(\"mean adv\", np.mean(batch_adv_n))\n",
    "    print(\"iteration time\", time.time() - tic)\n",
    "#     print(paths[0])\n",
    "    \n",
    "    \n",
    "    expert_player = Players.Expert_Player()\n",
    "    _, expert_batch_winners = batch_rollout(player, expert_player, env, max_time_steps=900, exploration_on =True, )\n",
    "    player_loss_percentage_vs_expert = expert_batch_winners[2]*1.0/(expert_batch_winners[0] + expert_batch_winners[1] + expert_batch_winners[2])\n",
    "    print(\"loss percent vs expert\", player_loss_percentage_vs_expert)\n",
    "    opponent = Players.NN_Player(model, model_input_s, sess, observation_placeholder,mask_placeholder)\n",
    "            \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./bot_10_28_v6.ckpt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_file_name = './bot_10_28_v7.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./bot_10_28_v3.ckpt\n"
     ]
    }
   ],
   "source": [
    "temp_file_name = './bot_10_28_v3.ckpt'\n",
    "\n",
    "#Want to duplicate session\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, temp_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Say something: 2\n",
      "[2]\n",
      "1\n",
      "[[0. 0. 1.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 0.]]\n",
      "Say something: 7\n",
      "[7]\n",
      "1\n",
      "[[2. 0. 1.]\n",
      " [0. 2. 0.]\n",
      " [0. 1. 0.]]\n",
      "Say something: 8\n",
      "[8]\n",
      "1\n",
      "[[2. 2. 1.]\n",
      " [0. 2. 0.]\n",
      " [0. 1. 1.]]\n",
      "Say something: 3\n",
      "[3]\n",
      "1\n",
      "[[2. 2. 1.]\n",
      " [1. 2. 2.]\n",
      " [0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "human = Players.Human_Player()\n",
    "sample_trajectory(human,opponent,env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_log_prob() missing 1 required positional argument: 'mask_placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-9ea0df022140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#Define Loss functions *symbolically*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_and_update_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madv_n_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_log_prob() missing 1 required positional argument: 'mask_placeholder'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess = tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(new_model, action_placeholder, mask_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "        #Produce some trajectories\n",
    "        paths = sample_trajectories_random_opponent(new_model, sess, 20, env = env)\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"model\" +str(-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(player, opponent, env):\n",
    "    \"\"\"player:   realization of Player.Player abstract class\n",
    "       opponent: realization of Player.Player abstract class\n",
    "       env:      environment which follows open ai gym environment structure and has a current_player int either 1 or 2\n",
    "    \n",
    "       realizes a rollout of env using player and opponent policy\n",
    "       \n",
    "       ouputs a path dictionary with keys: observation, reward, action\n",
    "       Each yields a 1D np array which shows the observation, reward, action pair at every point of the rollout\n",
    "    \"\"\"\n",
    "    \n",
    "    obs, acs, rewards = [], [], []\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        if env.current_player == 1:\n",
    "            obs.append(ob)\n",
    "            player_action = player.policy(np.array([ob]), env.legal_moves)\n",
    "            acs.append(player_action[0])\n",
    "            ob, rew, done, _ = env.step(player_action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        else:\n",
    "            opponent_action = opponent.policy(np.array([ob]), env.legal_moves())\n",
    "            ob, rew, done, _ = env.step(opponent_action[0])\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "    return path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 867, 1: 0, 2: 9133}\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "reload(Players)\n",
    "player = Players.Random_Player()\n",
    "opponent = Players.Expert_Player()\n",
    "\n",
    "env = TicTacToe.TicTacToe()\n",
    "batch_winners = {0:0,1:0,2:0}\n",
    "for i in range(10000):\n",
    "    path = sample_trajectory(player,opponent,env)\n",
    "    batch_winners[env.current_winner] += 1\n",
    "print(batch_winners)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing Q-learning....\n",
    "from importlib import reload \n",
    "reload(Players)\n",
    "\n",
    "def collect_tuples(model, observation_placeholder, opponent, env, sess, num_tuples =100, exploration =.1):\n",
    "    current_state1 = None\n",
    "    current_action = None\n",
    "    current_reward = None\n",
    "    current_state2 = None\n",
    "    tuples = [] #or maybe an array?\n",
    "    ob = env.reset\n",
    "    i=0\n",
    "    while i< num_tuples:\n",
    "        i+=1\n",
    "        #Record the current state\n",
    "        current_state1 = ob\n",
    "\n",
    "        #We pick the best action epsilon-greedily\n",
    "        action = sess.run( model, feed_dict = {observation_placeholder: [ob]})[0] #or something\n",
    "        if np.random.uniform() < exploration:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        #Record the current action and reward\n",
    "        ob = env.step(action)\n",
    "        current_action = action\n",
    "        current_reward = env.get_reward() #or whatever\n",
    "        \n",
    "        #Record the new state\n",
    "        current_state2 = ob\n",
    "        tuples.append({'state1': current_state1, 'action':current_action, 'state2':current_state2, 'reward':current_reward})\n",
    "        \n",
    "    return tuples\n",
    "    \n",
    "    \n",
    "def Q_learning_model(state1_placeholder, scope, num_actions = 9, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = tf.contrib.layers.flatten(state1_placeholder)\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        out = tf.layers.dense(out, 64  , bias_initializer = tf.zeros_initializer(), activation = tf.nn.softmax)\n",
    "        Q_func = tf.layers.dense(out, num_actions , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return Q_func\n",
    "\n",
    "\n",
    "def get_loss_and_optimizer(model, targets_placeholder):\n",
    "    diff = tf.subtract(tf.cast(model, tf.float64), tf.cast(targets_placeholder, tf.float64))\n",
    "    loss = tf.reduce_sum( tf.square( diff ), 1  ) \n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicating session to freeze weights for evaluation...\n",
      "INFO:tensorflow:Restoring parameters from ./to_duplicate.ckpt\n",
      "Destroying Q_Player and Session...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'method'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-13f41220a158>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#Collect some tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdatabase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdatabase\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcollect_tuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate1_placeholder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m###Training time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-23bf10df8bea>\u001b[0m in \u001b[0;36mcollect_tuples\u001b[0;34m(model, observation_placeholder, opponent, env, sess, num_tuples, exploration)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#We pick the best action epsilon-greedily\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mobservation_placeholder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#or something\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mexploration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \"\"\"\n\u001b[0;32m--> 492\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'method'"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "#Define any constants\n",
    "gamma = .95\n",
    "\n",
    "#Define the placeholders\n",
    "state1_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "state2_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "targets_placeholder = tf.placeholder(shape = [None, 9], dtype = tf.int32)\n",
    "mask_placeholder = tf.placeholder(shape = [None,9], dtype = tf.int32)\n",
    "\n",
    "\n",
    "#Define the model and loss function\n",
    "model = Q_learning_model(state1_placeholder, scope = \"firstQ\",  num_actions = 9, reuse=tf.AUTO_REUSE)\n",
    "_, update_op, _  = get_loss_and_optimizer(model, targets_placeholder)\n",
    "\n",
    "#Start a session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#Define the players\n",
    "player = Players.Q_Player(model, sess, state1_placeholder, mask_placeholder, duplicate=True)\n",
    "opponent = Players.Random_Player()\n",
    "\n",
    "#Collect some tuples\n",
    "database = []\n",
    "database += collect_tuples(model, state1_placeholder, opponent, env, sess)\n",
    "\n",
    "###Training time\n",
    "#I have a tuple, state-action-state from somewhere\n",
    "sampled_tuple = random.choice(database)\n",
    "\n",
    "not_updated_Q = sess.run(player.q_func(), feed_dict = {state1_placeholder: [sampled_tuple['state1']]})\n",
    "\n",
    "target_Q = not_updated_Q\n",
    "target_Q[0,sampled_tuple['action']] =  sampled_tuple['reward'] \n",
    "target_Q[0,sampled_tuple['action']] += tf.reduce_max(sess.run(player.q_function(), feed_dict = {state1_placeholder: [sampled_tuple['state2']]}), 1)[0]\n",
    "\n",
    "sess.run(update_op, feed_dict= {state1_placeholder: sampled_tuple['state1'], targets_placeholder: [target_Q] })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def get_target_values(player, gamma = .99):\n",
    "#     #sample from replay buffer\n",
    "#     #get form state-action-state\n",
    "#     initial_state = \n",
    "#     action = \n",
    "#     final_state =\n",
    "    \n",
    "#     state_action_value = player.policy(initial_state)[action]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
