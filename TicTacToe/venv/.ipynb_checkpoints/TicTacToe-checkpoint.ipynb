{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import time\n",
    "import pickle\n",
    "import sys\n",
    "import gym.spaces\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow                as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from collections import namedtuple\n",
    "import TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def TicTacToe_model(placeholder, num_actions, scope, reuse=tf.AUTO_REUSE):\n",
    "    # A model for a TicTacToe q-function\n",
    "    placeholder = tf.contrib.layers.flatten(placeholder)\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        out = placeholder\n",
    "        out = tf.cast(out, tf.float32)\n",
    "        out = tf.layers.dense(out, 64 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = tf.nn.relu)\n",
    "        out = tf.layers.dense(out, 9 , kernel_initializer = tf.zeros_initializer(), bias_initializer = tf.zeros_initializer(), activation = None)\n",
    "    return out\n",
    "\n",
    "    \n",
    "def update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter):\n",
    "    old_model = new_model\n",
    "    old_model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter-1))\n",
    "    \n",
    "    new_model = TicTacToe_model(board_placeholder, 9, scope = \"model\" + str(update_iter))\n",
    "    \n",
    "    new_model_vars =  tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"model\" +str(update_iter))\n",
    "    sess.run(tf.variables_initializer(new_model_vars))\n",
    "    \n",
    "    for var_old, var_new in zip(old_model_vars, new_model_vars):\n",
    "        var_new = sess.run(var_old)\n",
    "#        print(sess.run(var_old))\n",
    "#        print(sess.run(var_new))\n",
    "#    print(sess.run(old_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "#    print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "    return old_model, new_model\n",
    "\n",
    "def update_and_duplicate_old_model():\n",
    "    old_model = new_model\n",
    "    \n",
    "    variables = tf.trainable_variables()\n",
    "    for var1 in variables:\n",
    "        if \"pred/\" in var1.name:\n",
    "            trained_var = [var2 for var2 in tf.global_variables() if var2.op.name in str.replace(var1.name, \"pred/\", \"\")][0]\n",
    "            value = sess.run(trained_var)\n",
    "            sess.run(tf.assign(var1, value))\n",
    "\n",
    "\n",
    "def sample_action(model):\n",
    "    out = model\n",
    "    dist = tf.distributions.Categorical(logits=out)\n",
    "    return dist.sample()\n",
    "    \n",
    "    \n",
    "\n",
    "def sample_trajectory(old_model, new_model, sess, env):\n",
    "    new_sample = sample_action(new_model)\n",
    "    old_sample = sample_action(old_model)\n",
    "    obs, acs, rewards = [], [], []\n",
    "    seed = np.random.randint(0,2)\n",
    "    ob = env.reset()\n",
    "    if seed % 2 == 0:\n",
    "        while True:\n",
    "            obs.append(ob)\n",
    "            action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "            acs.append(action)\n",
    "            ob, rew, done, _ = env.step(action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "            ob, rew, done, _ = env.step(action[0])\n",
    "    else:\n",
    "        while True:       \n",
    "            action = sess.run(old_sample, feed_dict={board_placeholder: [ob]})\n",
    "            ob, rew, done, _ = env.step(action[0])\n",
    "   \n",
    "            obs.append(ob)\n",
    "            action = sess.run(new_sample, feed_dict={board_placeholder: [ob]})\n",
    "            acs.append(action)\n",
    "            ob, rew, done, _ = env.step(action[0])\n",
    "            rewards.append(rew)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "    path = {\"observation\" : np.array(obs, dtype=np.float32), \n",
    "                \"reward\" : np.array(rewards, dtype=np.float32), \n",
    "                \"action\" : np.array(acs, dtype=np.float32)}\n",
    "#    print(str(len(path[\"observation\"])))\n",
    "    return path\n",
    "\n",
    "def sample_trajectories(old_model, new_model, sess, min_timesteps_per_batch, env):\n",
    "    paths =[]\n",
    "    timesteps_this_batch = 0\n",
    "    while True:\n",
    "        path = sample_trajectory(old_model, new_model, sess, env)\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += len(path['observation'])\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    return paths\n",
    "    \n",
    "def sum_of_rewards(paths, gamma = .95): \n",
    "    re_n = [path[\"reward\"] for path in paths]\n",
    "    q_n = []\n",
    "    for seq_of_rewards in re_n:\n",
    "        for t in range(len(seq_of_rewards)):\n",
    "            weighted_sequence = seq_of_rewards[t:] * np.array([gamma**i for i in range(len(seq_of_rewards[t:]))])\n",
    "            q_n.append(np.sum(weighted_sequence))\n",
    "    adv_n = q_n\n",
    "    return adv_n\n",
    "        \n",
    "def standardize_advantage(adv_n):\n",
    "    adv_n = (adv_n - np.mean(adv_n)) \n",
    "    adv_n = adv_n * (1.0/(np.std(adv_n)+.0000001))\n",
    "    return adv_n\n",
    "\n",
    "def get_log_prob(model, action_placeholder):\n",
    "    logits = model\n",
    "    \n",
    "    log_prob = -tf.nn.sparse_softmax_cross_entropy_with_logits(labels= action_placeholder, logits= logits)\n",
    "    return log_prob\n",
    "\n",
    "def loss_and_update_op(log_prob,adv_n):\n",
    "    loss = -tf.reduce_mean(log_prob * adv_n) \n",
    "    optimizer = tf.train.AdamOptimizer(5e-3)\n",
    "    update_op = optimizer.minimize(loss)\n",
    "    return loss, update_op, optimizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "for k in range(number_expert_updates):\n",
    "    print(\"iteration number\", k)\n",
    "    #Define Loss functions *symbolically*\n",
    "    log_prob = get_log_prob(new_model, action_placeholder)\n",
    "    loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "    #Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "    if k ==0:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "    else:\n",
    "        sess.run(tf.variables_initializer(optimizer.variables()))\n",
    "    \n",
    "    for i in range(number_updates_per_expert_update):\n",
    "        #Produce some trajectories\n",
    "#        print(\"sampling games...\")\n",
    "        paths = sample_trajectories(old_model, new_model, sess, 20, env = env)\n",
    "#        print(\"games sampled.\")\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "#        print(\"Last game played\", paths)\n",
    "        \n",
    "        #Update the model\n",
    "#        print(\"updating model...\")\n",
    "#        print(sess.run(new_model, feed_dict = {board_placeholder: [[[1,0,0],[0,0,0],[0,0,0]]]})[0,:3])\n",
    "        test_vars =  tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"model\" +str(k-1))\n",
    "        for var in test_vars:\n",
    "            print(var.name, np.max(sess.run(var)))\n",
    "            \n",
    "#        for i, var in enumerate(test_vars):\n",
    "#            print(var.name, sess.run(var))\n",
    "\n",
    "        sess.run(update_op, feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n",
    "#        print(\"model updated.\")\n",
    "    #if new_model does better...\n",
    "    old_model, new_model = update_old_model_initialize_new_model(old_model, new_model, board_placeholder, sess, update_iter= k)\n",
    "    \n",
    "\n",
    "    \n",
    "for path in paths:\n",
    "    print(path['observation'])\n",
    "    print(path['action'])\n",
    "    print(path['reward'])\n",
    "    print(\"____\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(paths[0]['observation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(a.name, sess.run(a)) for i, a  in enumerate(tf.trainable_variables())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.trainable_variables()[16].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#define the board, models\n",
    "board_placeholder = tf.placeholder(shape = [None, 3,3], dtype = tf.int32)\n",
    "adv_n_placeholder = tf.placeholder(shape = [None], dtype = tf.float32)\n",
    "action_placeholder = tf.placeholder(shape = [None], dtype = tf.int32)\n",
    "new_model = TicTacToe_model(board_placeholder, 9, scope = \"model-1\", reuse=tf.AUTO_REUSE)\n",
    "old_model = TicTacToe_model(board_placeholder, 9, scope = \"model-2\", reuse=tf.AUTO_REUSE)\n",
    "\n",
    "#start a session\n",
    "sess =tf.Session()\n",
    "\n",
    "#start an environment\n",
    "env = TicTacToe.TicTacToe()\n",
    "\n",
    "number_updates_per_expert_update = 10\n",
    "number_expert_updates = 4\n",
    "\n",
    "#Define Loss functions *symbolically*\n",
    "log_prob = get_log_prob(new_model, action_placeholder)\n",
    "loss, update_op, optimizer = loss_and_update_op(log_prob, adv_n_placeholder)\n",
    "\n",
    "#Initialize all variables first time only, otherwise update only uptimizer vars\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(20):\n",
    "        #Produce some trajectories\n",
    "        paths = sample_trajectories(old_model, new_model, sess, 20, env = env)\n",
    "        adv_n = sum_of_rewards(paths) \n",
    "        boards = np.concatenate([path['observation'] for path in paths])\n",
    "        actions = np.squeeze(np.concatenate([path[\"action\"] for path in paths])).astype(int)\n",
    "        \n",
    "        #Update the model\n",
    "        loss_current = sess.run([update_op,loss], feed_dict = {adv_n_placeholder: adv_n, board_placeholder: boards , action_placeholder: actions})\n",
    "\n",
    "        print(loss_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_low_tower  = 24\n",
    "height_low_tower = 77\n",
    "\n",
    "n = width_low_tower\n",
    "k = height_low_tower\n",
    "\n",
    "withinLevels  = 0\n",
    "betweenLevels = 0\n",
    "for j in range(1,n):\n",
    "    withinLevels += 2*j*(j-1)\n",
    "    betweenLevels += 4*j*j\n",
    "    \n",
    "pyramid = withinLevels + betweenLevels\n",
    "\n",
    "lower_tower_within = 2*k*n*(n-1)\n",
    "lower_tower_between = (k-1)*n*n\n",
    "\n",
    "lower_tower = lower_tower_within + lower_tower_between\n",
    "\n",
    "    \n",
    "print(pyramid+lower_tower)\n",
    "print(withinLevels + lower_tower_within)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
